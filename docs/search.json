[{"path":"index.html","id":"prerequisites","chapter":"1 Prerequisites","heading":"1 Prerequisites","text":"","code":"\ninstall.packages(\"bookdown\")\n# or the development version\n# devtools::install_github(\"rstudio/bookdown\")"},{"path":"intro.html","id":"intro","chapter":"2 ML Models","heading":"2 ML Models","text":"","code":""},{"path":"intro.html","id":"linear-regression","chapter":"2 ML Models","heading":"2.1 Linear Regression","text":"Linear Regression involves finding ‘line best fit’ represents dataset using least squares method. least squares method involves finding linear equation minimizes sum squared residuals. residual equal actual minus predicted value.give example, red line better line best fit green line closer points, thus, residuals smaller.Linear Regression one fundamental algorithms used model relationships dependent variable one independent variables. simpler terms, involves finding ‘line best fit’ represents two variables.line best fit found minimizing squared distances points line best fit — known minimizing sum squared residuals. residual simply equal predicted value minus actual value.","code":""},{"path":"intro.html","id":"whats-the-difference-between-linear-regression-and-logistic-regression","chapter":"2 ML Models","heading":"2.2 What’s the difference between Linear Regression and Logistic Regression?","text":"Linear Regression used predict continuous variable mainly used solve regression problems. Linear regression finds best fit line output numerical value can predicted.Logistic Regression used predict categorical values mainly used classification problems. Logistic regression produces S curve classifies, output binary categories.","code":""},{"path":"intro.html","id":"what-is-overfitting","chapter":"2 ML Models","heading":"2.3 What is overfitting?","text":"Overfitting error model ‘fits’ data well, resulting model high variance low bias. consequence, overfit model inaccurately predict new data points even though high accuracy training data.Overfitting modeling error function fits data closely, resulting high levels error new data introduced model.number ways can prevent overfitting model:Cross-validation: Cross-validation technique used assess well model performs new independent dataset. simplest example cross-validation split data two groups: training data testing data, use training data build model testing data test model.Cross-validation: Cross-validation technique used assess well model performs new independent dataset. simplest example cross-validation split data two groups: training data testing data, use training data build model testing data test model.Regularization: Overfitting occurs models higher degree polynomials. Thus, regularization reduces overfitting penalizing higher degree polynomials.Regularization: Overfitting occurs models higher degree polynomials. Thus, regularization reduces overfitting penalizing higher degree polynomials.Reduce number features: can also reduce overfitting simply reducing number input features. can manually removing features, can use technique, called Principal Component Analysis, projects higher dimensional data (eg. 3 dimensions) smaller space (eg. 2 dimensions).Reduce number features: can also reduce overfitting simply reducing number input features. can manually removing features, can use technique, called Principal Component Analysis, projects higher dimensional data (eg. 3 dimensions) smaller space (eg. 2 dimensions).Ensemble Learning Techniques: Ensemble techniques take many weak learners converts strong learner bagging boosting. bagging boosting, techniques tend overfit less alternative counterparts.Ensemble Learning Techniques: Ensemble techniques take many weak learners converts strong learner bagging boosting. bagging boosting, techniques tend overfit less alternative counterparts.Overfitting error model ‘fits’ data well, resulting model high variance low bias. consequence, overfit model inaccurately predict new data points even though high accuracy training data.","code":""},{"path":"intro.html","id":"what-is-the-bias-variance-tradeoff","chapter":"2 ML Models","heading":"2.4 What is the bias-variance tradeoff?","text":"bias estimator difference expected value true value. model high bias tends oversimplified results underfitting. Variance represents model’s sensitivity data noise. model high variance results overfitting.Therefore, bias-variance tradeoff property machine learning models lower variance results higher bias vice versa. Generally, optimal balance two can found error minimized.","code":""},{"path":"intro.html","id":"what-are-ridge-and-lasso-regression-and-what-are-the-differences-between-them","chapter":"2 ML Models","heading":"2.5 What are ridge and lasso regression and what are the differences between them?","text":"L1 L2 regularization methods used reduce overfitting training data. Least Squares minimizes sum squared residuals, can result low bias high variance.L2 Regularization, also called ridge regression, minimizes sum squared residuals plus lambda times slope squared. additional term called Ridge Regression Penalty. increases bias model, making fit worse training data, also decreases variance.take ridge regression penalty replace absolute value slope, get Lasso regression L1 regularization.L2 less robust stable solution always one solution. L1 robust unstable solution can possibly multiple solutions.","code":""},{"path":"intro.html","id":"whats-the-difference-between-l2-and-l1-regularization","chapter":"2 ML Models","heading":"2.6 What’s the difference between L2 and L1 regularization?","text":"Penalty terms: L1 regularization uses sum absolute values weights, L2 regularization uses sum weights squared.Penalty terms: L1 regularization uses sum absolute values weights, L2 regularization uses sum weights squared.Feature selection: L1 performs feature selection reducing coefficients predictors 0, L2 .\nComputational efficiency: L2 analytical solution, L1 .Feature selection: L1 performs feature selection reducing coefficients predictors 0, L2 .\nComputational efficiency: L2 analytical solution, L1 .Multicollinearity: L2 addresses multicollinearity constraining coefficient norm.Multicollinearity: L2 addresses multicollinearity constraining coefficient norm.L1 effectively removes features unimportant, aggressively can lead underfitting. L2 weighs feature instead removing entirely, can lead better accuracy. Briefly, L1 removes features L2 doesn’t, L2 regulates weights instead.L1 effectively removes features unimportant, aggressively can lead underfitting. L2 weighs feature instead removing entirely, can lead better accuracy. Briefly, L1 removes features L2 doesn’t, L2 regulates weights instead.","code":""},{"path":"intro.html","id":"whats-regularization-and-whats-the-difference-between-l1-and-l2-regularization","chapter":"2 ML Models","heading":"2.7 What’s Regularization? and what’s the difference between L1 and L2 regularization?","text":"Regularization machine learning process regularizing parameters constrain, regularizes, shrinks coefficient estimates towards zero. words, technique discourages learning complex flexible model, avoiding risk Overfitting. Regularization basically adds penalty model complexity increases can help avoid overfitting.","code":""},{"path":"intro.html","id":"ridge-regression","chapter":"2 ML Models","heading":"2.7.1 Ridge Regression","text":"Ridge regression, also known L2 Regularization, regression technique introduces small amount bias reduce overfitting. minimizing sum squared residuals plus penalty, penalty equal lambda times slope squared. Lambda refers severity penalty.Without penalty, line best fit steeper slope, means sensitive small changes X. introducing penalty, line best fit becomes less sensitive small changes X. idea behind ridge regression.","code":""},{"path":"intro.html","id":"lasso-regression","chapter":"2 ML Models","heading":"2.7.2 Lasso Regression","text":"Lasso Regression, also known L1 Regularization, similar Ridge regression. difference penalty calculated absolute value slope instead.","code":""},{"path":"intro.html","id":"can-we-use-l1-regularization-for-feature-selection","chapter":"2 ML Models","heading":"2.8 Can we use L1 regularization for feature selection?","text":"Yes, nature L1 regularization lead sparse coefficients features. Feature selection can done keeping features non-zero coefficients.","code":""},{"path":"intro.html","id":"when-do-we-need-to-perform-feature-normalization-for-linear-models-when-its-okay-not-to-do-it","chapter":"2 ML Models","heading":"2.9 When do we need to perform feature normalization for linear models? When it’s okay not to do it?","text":"Feature normalization necessary L1 L2 regularizations. idea methods penalize features relatively equally. can’t done effectively every feature scaled differently.Linear regression without regularization techniques can used without feature normalization. Also, regularization can help make analytical solution stable, — adds regularization matrix feature matrix inverting .","code":""},{"path":"intro.html","id":"logistic-regression","chapter":"2 ML Models","heading":"2.10 Logistic Regression","text":"Logistic Regression classification technique also finds ‘line best fit’. However, unlike linear regression line best fit found using least squares, logistic regression finds line (logistic curve) best fit using maximum likelihood. done y value can one zero.Logistic regression similar linear regression used model probability discrete number outcomes, typically two. example, might want predict whether person alive dead given age.glance, logistic regression sounds much complicated linear regression, really one extra step.First, calculate score using equation similar equation line best fit linear regression.extra step feeding score previously calculated sigmoid function get probability return. probability can converted binary output, either 1 0.find weights initial equation calculate score, methods like gradient descent maximum likelihood used. Since ’s beyond scope article, won’t go much detail, now know works!","code":""},{"path":"intro.html","id":"what-is-logistic-regression-or-state-an-example-when-you-have-used-logistic-regression-recently.","chapter":"2 ML Models","heading":"2.11 What is logistic regression? Or State an example when you have used logistic regression recently.","text":"Logistic Regression often referred logit model technique predict binary outcome linear combination predictor variables. example, want predict whether particular political leader win election . case, outcome prediction binary .e. 0 1 (Win/Lose). predictor variables amount money spent election campaigning particular candidate, amount time spent campaigning, etc.","code":""},{"path":"metrics.html","id":"metrics","chapter":"3 Metrics","heading":"3 Metrics","text":"","code":""},{"path":"metrics.html","id":"accuracy","chapter":"3 Metrics","heading":"3.1 Accuracy:","text":"(TP+TN/(TP+FP+FN+TN): Accuracy proportion true results among total number cases examined.: Accuracy proportion true results among total number cases examined.: Accuracy valid choice evaluation classification problems well balanced skewed class imbalance.: Accuracy valid choice evaluation classification problems well balanced skewed class imbalance.AWARE: predicting asteroid hit earth? Just say time. 99% accurate. model can reasonably accurate, valuableAWARE: predicting asteroid hit earth? Just say time. 99% accurate. model can reasonably accurate, valuable","code":""},{"path":"metrics.html","id":"precision","chapter":"3 Metrics","heading":"3.2 Precision:","text":"(TP)/(TP+FP): answers question: proportion predicted Positives truly Positive?\nAsteroid problem, never predicted true positive, precision = 0\n: answers question: proportion predicted Positives truly Positive?Asteroid problem, never predicted true positive, precision = 0WHEN: Precision valid choice evaluation metric want sure prediction. example: building system predict decrease credit limit particular account, want sure prediction may result customer dissatisfaction.: Precision valid choice evaluation metric want sure prediction. example: building system predict decrease credit limit particular account, want sure prediction may result customer dissatisfaction.AWARE: precise means model leave lot credit defaulters untouched hence lose money.AWARE: precise means model leave lot credit defaulters untouched hence lose money.","code":""},{"path":"metrics.html","id":"recall","chapter":"3 Metrics","heading":"3.3 Recall:","text":"(TP)/(TP+FN): answers different question: proportion actual Positives correctly classified?\nAsteroid problem, never predicted true positive, recall = 0\n: answers different question: proportion actual Positives correctly classified?Asteroid problem, never predicted true positive, recall = 0WHEN: Recall valid choice evaluation metric want capture many positives possible. example: building system predict person cancer , want capture disease even sure.: Recall valid choice evaluation metric want capture many positives possible. example: building system predict person cancer , want capture disease even sure.AWARE: Recall 1 predict 1 examples.AWARE: Recall 1 predict 1 examples.","code":""},{"path":"metrics.html","id":"f1-score","chapter":"3 Metrics","heading":"3.4 F1 Score:","text":"2 (precisionrecall)/(precision + recall): utilize tradeoff precision vs. recall. F1 score number 0 1 harmonic mean precision recall.\nF1 score number 0 1 harmonic mean precision recall.\npredicting asteroid hit earth .\nsay “” whole training set. precision 0. recall positive class? zero. accuracy? 99%.\nhence F1 score also 0. thus get know classifier accuracy 99% basically worthless case. hence solves problem.\n: utilize tradeoff precision vs. recall. F1 score number 0 1 harmonic mean precision recall.F1 score number 0 1 harmonic mean precision recall.F1 score number 0 1 harmonic mean precision recall.predicting asteroid hit earth .predicting asteroid hit earth .say “” whole training set. precision 0. recall positive class? zero. accuracy? 99%.say “” whole training set. precision 0. recall positive class? zero. accuracy? 99%.hence F1 score also 0. thus get know classifier accuracy 99% basically worthless case. hence solves problem.hence F1 score also 0. thus get know classifier accuracy 99% basically worthless case. hence solves problem.: want model good precision recall.: want model good precision recall.F1 score sort maintains balance precision recall classifier. precision low, F1 low recall low F1 score low.F1 score sort maintains balance precision recall classifier. precision low, F1 low recall low F1 score low.EXAMPLE: police inspector want catch criminals, want sure person catch criminal (Precision) also want capture many criminals (Recall) possible. F1 score manages tradeoff.EXAMPLE: police inspector want catch criminals, want sure person catch criminal (Precision) also want capture many criminals (Recall) possible. F1 score manages tradeoff.AWARE: main problem F1 score gives equal weight precision recall. might sometimes need include domain knowledge evaluation want recall precision.solve , can creating weighted F1 metric beta manages tradeoff precision recall.\nROC stands Receiver Operating Characteristics. diagrammatic representation shows contrast true positive rate vs false positive rate. used need predict probability binary outcome. AUC area ROC curve. represents degree separability. used value much model capable distinguishing classes. Higher value better (0,1)\nAWARE: main problem F1 score gives equal weight precision recall. might sometimes need include domain knowledge evaluation want recall precision.solve , can creating weighted F1 metric beta manages tradeoff precision recall.ROC stands Receiver Operating Characteristics. diagrammatic representation shows contrast true positive rate vs false positive rate. used need predict probability binary outcome. AUC area ROC curve. represents degree separability. used value much model capable distinguishing classes. Higher value better (0,1)","code":""},{"path":"metrics.html","id":"which-metrics-to-use-when-classification","chapter":"3 Metrics","heading":"3.5 Which metrics to use when (classification)?","text":"Accuracy, Precision, Recall:","code":""},{"path":"metrics.html","id":"accuracy-1","chapter":"3 Metrics","heading":"3.5.1 Accuracy","text":"Accuracy quintessential classification metric. pretty easy understand. easily suited binary well multiclass classification problem.Accuracy = (TP+TN)/(TP+FP+FN+TN)Accuracy proportion true results among total number cases examined.use?Accuracy valid choice evaluation classification problems well balanced skewed class imbalance.CaveatsLet us say target class sparse. want accuracy metric model performance? predicting asteroid hit earth? Just say time. 99% accurate. model can reasonably accurate, valuable.","code":""},{"path":"metrics.html","id":"precision-1","chapter":"3 Metrics","heading":"3.5.2 Precision","text":"Let’s start precision, answers following question: proportion predicted Positives truly Positive?Precision = (TP)/(TP+FP)asteroid prediction problem, never predicted true positive.thus precision=0When use?Precision valid choice evaluation metric want sure prediction. example: building system predict decrease credit limit particular account, want sure prediction may result customer dissatisfaction.CaveatsBeing precise means model leave lot credit defaulters untouched hence lose money.","code":""},{"path":"metrics.html","id":"recall-1","chapter":"3 Metrics","heading":"3.5.3 Recall","text":"Another useful measure recall, answers different question: proportion actual Positives correctly classified?Recall = (TP)/(TP+FN)asteroid prediction problem, never predicted true positive.thus recall also equal 0.use?Recall valid choice evaluation metric want capture many positives possible. example: building system predict person cancer , want capture disease even sure.CaveatsRecall 1 predict 1 examples.\nthus comes idea utilizing tradeoff precision vs. recall — F1 Score.","code":""},{"path":"metrics.html","id":"f1-score-1","chapter":"3 Metrics","heading":"3.5.4 F1 Score","text":"favorite evaluation metric tend use lot classification projects.\nF1 score number 0 1 harmonic mean precision recall.Let us start binary prediction problem. predicting asteroid hit earth .\nsay “” whole training set. precision 0. recall positive class? zero. accuracy? 99%.\nhence F1 score also 0. thus get know classifier accuracy 99% basically worthless case. hence solves problem.use?want model good precision recall.","code":""},{"path":"metrics.html","id":"precision-recall-tradeoff","chapter":"3 Metrics","heading":"3.5.5 Precision-Recall Tradeoff","text":"Simply stated F1 score sort maintains balance precision recall classifier. precision low, F1 low recall low F1 score low.police inspector want catch criminals, want sure person catch criminal (Precision) also want capture many criminals (Recall) possible. F1 score manages tradeoff.Use?can calculate F1 score binary prediction problems using:one functions use get best threshold maximizing F1 score binary predictions. function iterates possible threshold values find one gives best F1 score.CaveatsThe main problem F1 score gives equal weight precision recall. might sometimes need include domain knowledge evaluation want recall precision.solve , can creating weighted F1 metric beta manages tradeoff precision recall.give \\(\\beta\\) times much importance recall precision.F1 Score can also used Multiclass problems.","code":"\n# from sklearn.metrics import f1_score\n\n# y_true = [0, 1, 1, 0, 1, 1]\n# y_pred = [0, 0, 1, 0, 0, 1]\n# f1_score(y_true, y_pred)  \n## y_pred is an array of predictions\n# def bestThresshold(y_true,y_pred):\n#    best_thresh = None\n#    best_score = 0\n#    for thresh in np.arange(0.1, 0.501, 0.01):\n#        score = f1_score(y_true, np.array(y_pred)>thresh)\n#        if score > best_score:\n#            best_thresh = thresh\n#            best_score = score\n#    return best_score , best_thresh\n# from sklearn.metrics import fbeta_score\n \n# y_true = [0, 1, 1, 0, 1, 1]\n# y_pred = [0, 0, 1, 0, 0, 1]\n# fbeta_score(y_true, y_pred,beta=0.5)"},{"path":"metrics.html","id":"log-lossbinary-crossentropy","chapter":"3 Metrics","heading":"3.5.6 Log Loss/Binary Crossentropy","text":"Log loss pretty good evaluation metric binary classifiers sometimes optimization objective well case Logistic regression Neural Networks.\nBinary Log loss example given formula p probability predicting 1.can see log loss decreases fairly certain prediction 1 true label 1.Use?output classifier prediction probabilities. Log Loss takes account uncertainty prediction based much varies actual label. gives us nuanced view performance model. general, minimizing Log Loss gives greater accuracy classifier.Use?CaveatsIt susceptible case imbalanced datasets. might introduce class weights penalize minority errors may use balancing dataset.","code":"\n# from sklearn.metrics import log_loss  \n## where y_pred are probabilities and y_true are binary class labels\n# log_loss(y_true, y_pred, eps=1e-15)"},{"path":"metrics.html","id":"categorical-cross-entropy","chapter":"3 Metrics","heading":"3.5.7 Categorical Cross Entropy","text":"log loss also generalizes multiclass problem. classifier multiclass setting must assign probability class examples. N samples belonging M classes, Categorical Crossentropy summation -ylogp values:y_ij 1 sample belongs class j else 0y_ij 1 sample belongs class j else 0p_ij probability classifier predicts sample belonging class j.p_ij probability classifier predicts sample belonging class j.Use?output classifier multiclass prediction probabilities. generally use Categorical Crossentropy case Neural Nets. general, minimizing Categorical cross-entropy gives greater accuracy classifier.Use?Caveats:susceptible case imbalanced datasets.","code":"\n# from sklearn.metrics import log_loss  \n## Where y_pred is a matrix of probabilities with shape = (n_samples, n_classes) and y_true is an array of class labels\n# log_loss(y_true, y_pred, eps=1e-15)"},{"path":"metrics.html","id":"auc","chapter":"3 Metrics","heading":"3.5.8 AUC","text":"AUC area ROC curve.AUC ROC indicates well probabilities positive classes separated negative classes","code":""},{"path":"metrics.html","id":"roc-curve","chapter":"3 Metrics","heading":"3.5.9 ROC curve","text":"got probabilities classifier. can use various threshold values plot sensitivity(TPR) (1-specificity)(FPR) cure ROC curve; True positive rate TPR just proportion trues capturing using algorithm.Sensitivity = TPR(True Positive Rate)Recall = TP/(TP+FN)False positive rate FPR just proportion false capturing using algorithm.1- Specificity = FPR(False Positive Rate)= FP/(TN+FP)can use ROC curves decide Threshold value.choice threshold value also depend classifier intended used.cancer classification application don’t want threshold big 0.5. Even patient 0.3 probability cancer classify 1.Otherwise, application reducing limits credit card, don’t want threshold less 0.5. little worried negative effect decreasing limits customer satisfaction.Use?AUC scale-invariant. measures well predictions ranked, rather absolute values. , example, marketer want find list users respond marketing campaign. AUC good metric use since predictions ranked probability order create list users send marketing campaign.Another benefit using AUC classification-threshold-invariant like log loss. measures quality model’s predictions irrespective classification threshold chosen, unlike F1 score accuracy depend choice threshold.Use?CaveatsSometimes need well-calibrated probability outputs models AUC doesn’t help .","code":"\n# import numpy as np\n# from sklearn.metrics import roc_auc_score\n# y_true = np.array([0, 0, 1, 1])\n# y_scores = np.array([0.1, 0.4, 0.35, 0.8])\n# print(roc_auc_score(y_true, y_scores))"},{"path":"metrics.html","id":"what-is-the-difference-between-precision-and-recall","chapter":"3 Metrics","heading":"3.6 What is the difference between precision and recall?","text":"Recall attempts answer “proportion actual positives identified correctly?”Precision attempts answer “proportion positive identifications actually correct?”","code":""},{"path":"metrics.html","id":"precision-recall-trade-off","chapter":"3 Metrics","heading":"3.7 Precision-recall trade-off","text":"Tradeoff means increasing one parameter lead decreasing . Precision-recall tradeoff occur due increasing one parameter(precision recall) keeping model .ideal scenario perfectly separable data, precision recall can get maximum value 1.0. practical situations, noise dataset dataset perfectly separable. might points positive class closer negative class vice versa. cases, shifting decision boundary can either increase precision recall . Increasing one parameter leads decreasing .","code":""},{"path":"metrics.html","id":"what-is-the-roc-curve-when-to-use-it","chapter":"3 Metrics","heading":"3.8 What is the ROC curve? When to use it?","text":"ROC stands Receiver Operating Characteristics. diagrammatic representation shows contrast true positive rate vs false positive rate. used need predict probability binary outcome.","code":""},{"path":"metrics.html","id":"what-is-auc-au-roc-when-to-use-it","chapter":"3 Metrics","heading":"3.9 What is AUC (AU ROC)? When to use it?","text":"AUC stands Area ROC Curve. ROC probability curve AUC represents degree measure separability. ’s used need value much model capable distinguishing classes. value 0 1, higher better.","code":""},{"path":"metrics.html","id":"we-have-two-models-one-with-85-accuracy-one-82.-which-one-do-you-pick","chapter":"3 Metrics","heading":"3.10 We have two models, one with 85% accuracy, one 82%. Which one do you pick?","text":"care accuracy model choose one 85%. interviewer ask , probably good idea get context, .e. model trying predict. give us better idea whether evaluation metric indeed accuracy another metric like recall f1 score.","code":""},{"path":"metrics.html","id":"is-accuracy-always-a-good-metric","chapter":"3 Metrics","heading":"3.11 Is accuracy always a good metric?","text":"Accuracy good performance metric imbalance dataset. example, binary classification 95% class 5% B class, constant prediction class accuracy 95%. case imbalance dataset, need choose Precision, recall, F1 Score depending problem trying solve.","code":""},{"path":"metrics.html","id":"what-is-a-confusion-matrix","chapter":"3 Metrics","heading":"3.12 What is a confusion matrix?","text":"confusion matrix, also known error matrix, summarized table used assess performance classification model. number correct incorrect predictions summarized count values broken class.True positive(TP) — Correct positive predictionTrue positive(TP) — Correct positive predictionFalse positive(FP) — Incorrect positive predictionFalse positive(FP) — Incorrect positive predictionTrue negative(TN) — Correct negative predictionTrue negative(TN) — Correct negative predictionFalse negative(FN) — Incorrect negative predictionFalse negative(FN) — Incorrect negative prediction","code":""},{"path":"metrics.html","id":"how-to-check-if-the-regression-model-fits-the-data-well","chapter":"3 Metrics","heading":"3.13 How to check if the regression model fits the data well?","text":"couple metrics can use:R-squared/Adjusted R-squared: Relative measure fit. explained previous answerR-squared/Adjusted R-squared: Relative measure fit. explained previous answerF1 Score: Evaluates null hypothesis regression coefficients equal zero vs alternative hypothesis least one doesn’t equal zeroF1 Score: Evaluates null hypothesis regression coefficients equal zero vs alternative hypothesis least one doesn’t equal zeroRMSE: Absolute measure fit.RMSE: Absolute measure fit.","code":""},{"path":"metrics.html","id":"model-evaluation","chapter":"3 Metrics","heading":"3.14 Model Evaluation","text":"evaluate regression model, can calculate R-squared, tells us much variability data model accounts . example, model R-squared 80%, 80% variation data can explained model.adjusted R-squared modified version r-squared adjusts number predictors model; increases new term improves model expected chance vice versa.","code":""},{"path":"clustering.html","id":"clustering","chapter":"4 Clustering","heading":"4 Clustering","text":"describe methods chapter.Math can added body using usual syntax like ","code":""},{"path":"clustering.html","id":"math-example","chapter":"4 Clustering","heading":"4.1 math example","text":"\\(p\\) unknown expected around 1/3. Standard error approximated\\[\nSE = \\sqrt(\\frac{p(1-p)}{n}) \\approx \\sqrt{\\frac{1/3 (1 - 1/3)} {300}} = 0.027\n\\]can also use math footnotes like this1.approximate standard error 0.0272","code":""},{"path":"data-preprocessing.html","id":"data-preprocessing","chapter":"5 Data Preprocessing","heading":"5 Data Preprocessing","text":"","code":""},{"path":"data-preprocessing.html","id":"what-do-you-mean-by-feature-splitting","chapter":"5 Data Preprocessing","heading":"5.1 What do you mean by Feature Splitting?","text":"feature splitting approach generate new features existing one improve performance model.Example, splitting names objects two parts- first last names.","code":""},{"path":"data-preprocessing.html","id":"what-are-the-feature-selection-methods-used-to-select-the-right-variables","chapter":"5 Data Preprocessing","heading":"5.2 What are the feature selection methods used to select the right variables?","text":"two types methods feature selection: filter methods wrapper methods.Filter methods include following:Linear discrimination analysisLinear discrimination analysisANOVAANOVAChi-SquareChi-SquareWrapper methods include following:Forward Selection: test one feature time keep adding get good fitForward Selection: test one feature time keep adding get good fitBackward Selection: test features start removing see works betterBackward Selection: test features start removing see works better","code":""},{"path":"data-preprocessing.html","id":"how-do-you-select-the-important-features-while-working-on-a-dataset","chapter":"5 Data Preprocessing","heading":"5.3 How do you select the important features while working on a dataset?","text":"various methods select important features data set include following:Multicollinearity: Identify discard correlated variables finalizing important variables.Multicollinearity: Identify discard correlated variables finalizing important variables.Using Linear Regression model: explanatory variables selected based p-values obtained Linear Regression.Using Linear Regression model: explanatory variables selected based p-values obtained Linear Regression.Wrapper Methods: Forward, Backward, Stepwise selectionWrapper Methods: Forward, Backward, Stepwise selectionRegularization: Lasso RegressionRegularization: Lasso RegressionEnsemble Technique: Apply Random Forest plot variable chartEnsemble Technique: Apply Random Forest plot variable chartInformation Gain: Top features can selected based information gain available set features.Information Gain: Top features can selected based information gain available set features.","code":""},{"path":"data-preprocessing.html","id":"what-are-some-of-the-steps-for-data-wrangling-and-data-cleaning-before-applying-machine-learning-algorithms","chapter":"5 Data Preprocessing","heading":"5.4 What are some of the steps for data wrangling and data cleaning before applying machine learning algorithms?","text":"many steps can taken data wrangling data cleaning. common steps listed :Data profiling: Almost everyone starts getting understanding dataset. specifically, can look shape dataset .shape description numerical variables .describe().Data profiling: Almost everyone starts getting understanding dataset. specifically, can look shape dataset .shape description numerical variables .describe().Data visualizations: Sometimes, ’s useful visualize data histograms, boxplots, scatterplots better understand relationships variables also identify potential outliers.Data visualizations: Sometimes, ’s useful visualize data histograms, boxplots, scatterplots better understand relationships variables also identify potential outliers.Syntax error: includes making sure ’s white space, making sure letter casing consistent, checking typos. can check typos using .unique() using bar graphs.Syntax error: includes making sure ’s white space, making sure letter casing consistent, checking typos. can check typos using .unique() using bar graphs.Standardization normalization: Depending dataset working machine learning method decide use, may useful standardize normalize data different scales different variables don’t negatively impact performance model.Standardization normalization: Depending dataset working machine learning method decide use, may useful standardize normalize data different scales different variables don’t negatively impact performance model.Handling null values: number ways handle null values including deleting rows null values altogether, replacing null values mean/median/mode, replacing null values new category (eg. unknown), predicting values, using machine learning models can deal null values. Read .Handling null values: number ways handle null values including deleting rows null values altogether, replacing null values mean/median/mode, replacing null values new category (eg. unknown), predicting values, using machine learning models can deal null values. Read .things include: removing irrelevant data, removing duplicates, type conversion.things include: removing irrelevant data, removing duplicates, type conversion.","code":""},{"path":"data-preprocessing.html","id":"what-are-the-missing-values-how-do-you-handle-missing-values","chapter":"5 Data Preprocessing","heading":"5.5 What are the missing values? How do you handle missing values?","text":"data cleaning process, can see lots missing values filled collected survey data collection. , handle missing values using following methods:Delete missing records: Ignoring rows dropping records.Delete missing records: Ignoring rows dropping records.Central Tendency: Fill values mean, mode, median.Central Tendency: Fill values mean, mode, median.Different mean different classes: Fill values using mean different classes, different means can used.Different mean different classes: Fill values using mean different classes, different means can used.Model Building: can also fill probable value using regression, Bayesian formula, decision tree, KNN, Prebuilt imputing libraries.Model Building: can also fill probable value using regression, Bayesian formula, decision tree, KNN, Prebuilt imputing libraries.Use constant Value: Fill constant value.Use constant Value: Fill constant value.fill values manually.fill values manually.","code":""},{"path":"data-preprocessing.html","id":"name-two-useful-methods-of-pandas-that-are-useful-to-handle-the-missing-values.","chapter":"5 Data Preprocessing","heading":"5.6 Name two useful methods of pandas that are useful to handle the missing values.","text":"two useful methods Pandas Library –isnull( )isnull( )dropna( )dropna( )help find columns data missing corrupted data drop values. cases want fill invalid values placeholder value 0, can use fillna() method.","code":""},{"path":"data-preprocessing.html","id":"give-several-ways-to-deal-with-missing-values","chapter":"5 Data Preprocessing","heading":"5.7 Give several ways to deal with missing values","text":"number ways handle null values including following:can omit rows null values altogetherYou can omit rows null values altogetherYou can replace null values measures central tendency (mean, median, mode) replace new category (eg. ‘None’)can replace null values measures central tendency (mean, median, mode) replace new category (eg. ‘None’)can predict null values based variables. example, row null value weight, value height, can replace null value average weight given height.can predict null values based variables. example, row null value weight, value height, can replace null value average weight given height.Lastly, can leave null values using machine learning model automatically deals null values.Lastly, can leave null values using machine learning model automatically deals null values.","code":""},{"path":"data-preprocessing.html","id":"explain-one-hot-encoding-and-label-encoding.-does-the-dimensionality-of-the-dataset-increase-or-decrease-after-applying-these-techniques","chapter":"5 Data Preprocessing","heading":"5.8 Explain One-hot encoding and Label Encoding. Does the dimensionality of the dataset increase or decrease after applying these techniques?","text":"One-hot encoding Label Encoding used encode categorical variables numerical variables can feed encoded data ML algorithms able find insights data.One-hot encoding representation categorical variables binary vectors Label Encoding converts labels words numeric form.One-hot encoding increases dimensionality data set Label encoding doesn’t affect dimensionality data set since One-hot encoding creates new variable level variable whereas, Label encoding, levels variable get encoded 1 0 replacement column.","code":""},{"path":"data-preprocessing.html","id":"why-do-we-need-one-hot-encoding","chapter":"5 Data Preprocessing","heading":"5.9 Why do we need one-hot encoding?","text":"simply encode categorical variables Label encoder, become ordinal can lead undesirable consequences. case, linear models treat category id 4 twice better category id 2. One-hot encoding allows us represent categorical variable numerical vector space ensures vectors category equal distances . approach suited situations, using categorical variables high cardinality (e.g. customer id) encounter problems come play curse dimensionality.","code":""},{"path":"data-preprocessing.html","id":"how-can-you-determine-which-features-are-the-most-important-in-your-model","chapter":"5 Data Preprocessing","heading":"5.10 How can you determine which features are the most important in your model?","text":"applied machine learning, success depends significantly quality data representation (features). Highly correlated features can make learning/sorting steps classification module easy. Conversely label classes complex function features, can impossible build good model [Dom 2012]. Thus -called feature engineering, process transforming data features relevant problem, often needed.feature selection scheme often involves techniques automatically select salient features large exploratory feature pool. Redundant irrelevant features well known cause poor accuracy discarding features first task. relevance often scored using mutual information calculation. Furthermore, input features thus offer high level discrimination classes. separability features can measured distance variance ratio classes. One recent work [Pham 2016] proposed systematic voting based feature selection data-driven approach incorporating criteria. can used common framework wide class problems.Another approach penalizing features important (e.g., yield high error metric) using regularization methods like Lasso Ridge.","code":""},{"path":"data-preprocessing.html","id":"how-can-you-choose-a-classifier-based-on-training-set-size","chapter":"5 Data Preprocessing","heading":"5.11 How can you choose a classifier based on training set size?","text":"training set small, high bias/low variance models (e.g. Naïve Bayes) tend perform better less likely overfit.training set large, low bias/high variance models (e.g. Logistic Regression) tend perform better can reflect complex relationships.","code":""},{"path":"data-preprocessing.html","id":"is-it-good-to-perform-scaling-before-the-split-or-after-the-split-by-keeping-train-and-test-split-criteria-in-mind","chapter":"5 Data Preprocessing","heading":"5.12 Is it good to perform scaling before the split or after the split by keeping train and test split criteria in mind?","text":"Ideally, need scaling post-train test split. Scaling post pre-split make much difference data closely packed.","code":""},{"path":"data-preprocessing.html","id":"when-can-a-categorical-value-be-treated-as-a-continuous-variable-and-what-effect-does-it-have-when-done-so","chapter":"5 Data Preprocessing","heading":"5.13 When can a categorical value be treated as a continuous variable and what effect does it have when done so?","text":"categorical independent variable can treated continuous one nature data points categorical data represents ordinal nature., independent variable ordinal data can treated continuous inclusion model increases performance model.","code":""},{"path":"data-preprocessing.html","id":"if-we-have-a-date-column-in-our-dataset-then-how-will-you-perform-feature-engineering","chapter":"5 Data Preprocessing","heading":"5.14 If we have a date column in our dataset, then how will you perform Feature Engineering?","text":"date column, can get lots important features day week, day month, day quarter, day year.Moreover, can extract date, month, year column also. features can impact prediction make model robust.Example, Sales business can impacted month day week.","code":""},{"path":"data-preprocessing.html","id":"how-would-you-handle-an-imbalanced-dataset","chapter":"5 Data Preprocessing","heading":"5.15 How would you handle an imbalanced dataset?","text":"imbalanced dataset , Example, classification problem statement let’s 90% data one class. result, accuracy 90% can skewed predictive power category data!suggestions get rid problems:collect dataset helps even imbalances dataset.collect dataset helps even imbalances dataset.Resample dataset correct imbalances.Resample dataset correct imbalances.Try different algorithm altogether dataset.Try different algorithm altogether dataset., ’s important keen sense damage unbalanced dataset can cause, balance .","code":""},{"path":"data-preprocessing.html","id":"how-should-you-deal-with-unbalanced-binary-classification","chapter":"5 Data Preprocessing","heading":"5.16 How should you deal with unbalanced binary classification?","text":"number ways handle unbalanced binary classification (assuming want identify minority class):First, want reconsider metrics ’d use evaluate model. accuracy model might best metric look ’ll use example explain . Let’s say 99 bank withdrawals fraudulent 1 withdrawal . model simply classified every instance “fraudulent”, accuracy 99%! Therefore, may want consider using metrics like precision recall.First, want reconsider metrics ’d use evaluate model. accuracy model might best metric look ’ll use example explain . Let’s say 99 bank withdrawals fraudulent 1 withdrawal . model simply classified every instance “fraudulent”, accuracy 99%! Therefore, may want consider using metrics like precision recall.Another method improve unbalanced binary classification increasing cost misclassifying minority class. increasing penalty , model classify minority class accurately.Another method improve unbalanced binary classification increasing cost misclassifying minority class. increasing penalty , model classify minority class accurately.Lastly, can improve balance classes oversampling minority class undersampling majority class.Lastly, can improve balance classes oversampling minority class undersampling majority class.","code":""},{"path":"data-preprocessing.html","id":"which-technique-would-you-use-in-cases-where-the-number-of-variables-is-greater-than-the-number-of-observations-in-the-dataset.-explain","chapter":"5 Data Preprocessing","heading":"5.17 Which technique would you use in cases where the number of variables is greater than the number of observations in the dataset. Explain?","text":"cases number variables greater number observations, represents high-dimensional dataset., possible calculate unique least-square coefficient estimate, therefore used penalized regression methods like Least Angle regression(LARS), LASSO, Ridge seems work well circumstances tend shrink coefficients order reduce variance.Moreover, situations higher variance least square estimates Ridge Regression works better.number variables greater number observations, represents high dimensional dataset. cases, possible calculate unique least square coefficient estimate. Penalized regression methods like LARS, Lasso Ridge seem work well circumstances tend shrink coefficients reduce variance. Whenever least square estimates higher variance, Ridge regression technique seems work best.","code":""},{"path":"data-preprocessing.html","id":"what-are-3-data-preprocessing-techniques-to-handle-outliers","chapter":"5 Data Preprocessing","heading":"5.18 What are 3 data preprocessing techniques to handle outliers?","text":"Winsorize (cap threshold)Winsorize (cap threshold)Transform reduce skew (using Box-Cox similar)Transform reduce skew (using Box-Cox similar)Remove outliers ’re certain anomalies measurement errors.Remove outliers ’re certain anomalies measurement errors.##weight one variable higher another, can say variable important?Yes - predictor variables normalized.\nWithout normalization, weight represents change output per unit change predictor. predictor huge range scale used predict output small range - example, using nation’s GDP predict maternal mortality rates - coefficient small. necessarily mean predictor variable important compared others.","code":""},{"path":"data-preprocessing.html","id":"how-do-we-handle-categorical-variables-in-decision-trees","chapter":"5 Data Preprocessing","heading":"5.19 How do we handle categorical variables in decision trees?","text":"decision tree algorithms can handle categorical variables box, others . However, can transform categorical variables, e.g. binary one-hot encoder.##happens correlated features data?random forest, since random forest samples features build tree, information contained correlated features twice much likely picked information contained features.general, adding correlated features, means linearly contains information thus reduce robustness model. time train model, model might pick one feature “job” .e. explain variance, reduce entropy, etc.","code":""},{"path":"data-preprocessing.html","id":"for-a-classification-problem-how-will-you-know-which-machine-learning-algorithm-to-choose","chapter":"5 Data Preprocessing","heading":"5.20 For a classification problem, how will you know which Machine Learning Algorithm to Choose?","text":"fixed rule thumb choose algorithm classification problem, following points can kept mind selecting algorithm:accuracy major focus: Test different algorithms cross-validate .accuracy major focus: Test different algorithms cross-validate .small training dataset: Use models low variance high bias.small training dataset: Use models low variance high bias.large training dataset: Use models high variance little bias.large training dataset: Use models high variance little bias.","code":""},{"path":"data-preprocessing.html","id":"how-much-data-will-you-allocate-for-your-training-validation-and-test-sets","chapter":"5 Data Preprocessing","heading":"5.21 How much data will you allocate for your training, validation and test sets?","text":"point answer question needs balance/equilibrium allocating data training, validation test sets.make training set small, actual model parameters might high variance. Also, test set small, chances unreliable estimation model performance. general thumb rule follow use 80: 20 train/test spilt. training set can split validation sets.","code":""},{"path":"data-preprocessing.html","id":"in-unsupervised-learning-if-a-ground-truth-about-a-dataset-is-unknown-how-can-we-determine-the-most-useful-number-of-clusters-to-be","chapter":"5 Data Preprocessing","heading":"5.22 In unsupervised learning, if a ground truth about a dataset is unknown, how can we determine the most useful number of clusters to be?","text":"supervised learning, number classes particular set data known outright, since data instance labeled member particular existent class. worst case, can scan class attribute count number unique entries exist.unsupervised learning, idea class attributes explicit class membership exist; fact, one dominant forms unsupervised learning – data clustering – aims approximate class membership minimizing interclass instance similarity maximizing intraclass similarity. major drawback clustering can requirement provide number classes exist unlabeled dataset onset, form another. lucky, may know data’s ground truth – actual number classes – beforehand. However, always case, numerous reasons, one may actually defined number classes (hence, clusters) data, whole point unsupervised learning task survey data attempt impose meaningful structure optimal cluster class numbers upon .","code":""},{"path":"data-preprocessing.html","id":"without-knowing-the-ground-truth-of-a-dataset-then-how-do-we-know-what-the-optimal-number-of-data-clusters-are","chapter":"5 Data Preprocessing","heading":"5.23 Without knowing the ground truth of a dataset, then, how do we know what the optimal number of data clusters are?","text":"one may expect, actually numerous methods go answering question. look 2 particular popular methods attempting answer question: elbow method silhouette method.","code":""},{"path":"data-preprocessing.html","id":"the-elbow-method","chapter":"5 Data Preprocessing","heading":"5.23.1 The Elbow Method","text":"elbow method often best place state, especially useful due ease explanation verification via visualization. elbow method interested explaining variance function cluster numbers (k k-means). plotting percentage variance explained k, first N clusters add significant information, explaining variance; yet, eventual value k result much less significant gain information, point graph provide noticeable angle. angle optimal number clusters, perspective elbow method.self-evident , order plot variance varying numbers clusters, varying numbers clusters must tested. Successive complete iterations clustering method must undertaken, results can plotted compared.","code":""},{"path":"data-preprocessing.html","id":"the-silhouette-method","chapter":"5 Data Preprocessing","heading":"5.23.2 The Silhouette Method","text":"silhouette method measures similarity object cluster – called cohesion – compared clusters – called separation. silhouette value means comparison, value range [-1, 1]; value close 1 indicates close relationship objects cluster, value close -1 indicates opposite. clustered set data model producing mostly high silhouette values likely acceptable appropriate model.","code":""},{"path":"data-preprocessing.html","id":"when-should-you-use-classification-over-regression","chapter":"5 Data Preprocessing","heading":"5.24 When should you use classification over regression?","text":"classification regression belong category supervised machine learning algorithms. However, target categorical, classification used whereas target variable continuous, regression used.\nclassification produces discrete values datasets strict categories, regression gives continuous results allow better distinguish differences individual points.order represent belonging data points certain categories, use classification regression.\nexample, wanted know whether name male female rather just correlated male female names.","code":""},{"path":"data-preprocessing.html","id":"how-can-we-use-an-unlabelled-datasetwithout-having-a-target-column-in-supervised-learning-algorithms","chapter":"5 Data Preprocessing","heading":"5.25 How can we use an unlabelled dataset(without having a target column) in Supervised Learning Algorithms?","text":"use dataset without output column, first give input dataset clustering algorithm, generates optimal number clusters, labels cluster numbers new target variable.Now, dataset independent dependent variables .e, target column present., completes objective apply supervised learning algorithms unlabeled data.","code":""},{"path":"data-preprocessing.html","id":"is-it-possible-to-test-the-probability-of-improving-the-model-accuracy-without-using-cross-validation-if-yes-please-explain.","chapter":"5 Data Preprocessing","heading":"5.26 Is it possible to test the probability of improving the model accuracy without using cross-validation? If yes, please explain.","text":"Yes, can test probability improving accuracy model without using cross-validation techniques., run ML model say K number iterations, recording accuracy. try plot accuracies remove 5% low probability values. , measure left [low]right [high] threshold, decided problem statement .\nNow, remaining 95% confidence, can say model can go low high .e, determine range.","code":""},{"path":"data-preprocessing.html","id":"what-cross-validation-technique-would-you-use-on-a-time-series-data-set.","chapter":"5 Data Preprocessing","heading":"5.27 What cross-validation technique would you use on a time series data set.","text":"Instead using k-fold cross-validation, aware fact time series randomly distributed data — inherently ordered chronological order.case time series data, use techniques like forward chaining — model past data look forward-facing data.fold 1: training[1], test[2]fold 1: training[1 2], test[3]fold 1: training[1 2 3], test[4]fold 1: training[1 2 3 4], test[5]","code":""},{"path":"data-preprocessing.html","id":"how-can-you-create-a-model-with-a-very-unbalanced-dataset-for-example-working-with-credit-card-fraud-data-and-there-are-very-few-real-fraud-cases-while-the-majority-of-the-cases-are-non-fraudulent.","chapter":"5 Data Preprocessing","heading":"5.28 How can you create a model with a very unbalanced dataset? For example, working with credit card fraud data and there are very few real fraud cases while the majority of the cases are non-fraudulent.","text":"Creating model unbalanced dataset yield bad results terms favoring training data, case non-fraudulent transactions. never create model unbalanced dataset. answer around trying gather balanced data possible oversample data using SMOTE (Synthetic Minority Sampling) Random Sampling (ROS).","code":""},{"path":"data-preprocessing.html","id":"smote-techniques-to-balance-datasets","chapter":"5 Data Preprocessing","heading":"5.28.1 SMOTE Techniques to Balance Datasets","text":"SMOTE technique creates new observations underrepresented class, case, fraudulent observations. synthetic observations almost identical original fraudulent observations. technique expeditious, types synthetic observations produces useful unique observations created oversampling techniques.","code":""},{"path":"neural-networks.html","id":"neural-networks","chapter":"6 Neural Networks","heading":"6 Neural Networks","text":"","code":""},{"path":"neural-networks.html","id":"briefly-explain-how-a-basic-neural-network-works","chapter":"6 Neural Networks","heading":"6.1 Briefly explain how a basic neural network works","text":"core, Neural Network essentially network mathematical equations. takes one input variables, going network equations, results one output variables.neural network, ’s input layer, one hidden layers, output layer. input layer consists one feature variables (input variables independent variables) denoted x1, x2, …, xn. hidden layer consists one hidden nodes hidden units. node simply one circles diagram . Similarly, output variable consists one output units.Like said beginning, neural network nothing network equations. node neural network composed two functions, linear function activation function. things can get little confusing, now, think linear function line best fit. Also, think activation function like light switch, results number 1 0.","code":""},{"path":"neural-networks.html","id":"activation-functions","chapter":"6 Neural Networks","heading":"6.2 Activation Functions","text":"activation function like light switch — determines whether neuron activated .several types activation functions, popular activation function Rectified Linear Unit function, also known ReLU function.","code":""},{"path":"neural-networks.html","id":"what-is-the-role-of-the-activation-function","chapter":"6 Neural Networks","heading":"6.2.1 What is the role of the activation function?","text":"purpose activation function introduce non-linearity output neuron. activation function decides whether neuron activated calculating weighted sum adding bias .","code":""},{"path":"neural-networks.html","id":"why-tanh-activation-function-preferred-over-sigmoid","chapter":"6 Neural Networks","heading":"6.2.2 Why Tanh activation function preferred over sigmoid?","text":"Tanh function called shifted version sigmoid function. output Tanh centers around 0 sigmoid’s around 0.5. Tanh Convergence usually faster average input variable training set close zero.struggle quickly find local global minimum, case Tanh can helpful faster convergence. derivatives Tanh larger Sigmoid causes faster optimization cost function. Tanh Sigmoid suffered vanishing gradient problems.","code":""},{"path":"neural-networks.html","id":"why-is-the-relu-activation-function-is-better-than-the-sigmoid-activation-function","chapter":"6 Neural Networks","heading":"6.2.3 Why is the ReLU activation function is better than the sigmoid activation function?","text":"Sigmoid function bounded 0 1. differentiable, non-linear, produces non-binary activations. problem Sigmoid vanishing gradients.ReLu(Rectified Linear Unit) like linearity switch. don’t need , “switch” . need , “switch” . ReLu avoids problem vanishing gradient.ReLu also provides benefit sparsity sigmoids result dense representations. Sparse representations useful dense representations.","code":""},{"path":"neural-networks.html","id":"what-is-the-use-of-the-leaky-relu-function","chapter":"6 Neural Networks","heading":"6.2.4 What is the use of the leaky ReLU function?","text":"main problem ReLU , differentiable 0 may result exploding gradients. resolve problem Leaky ReLu introduced differentiable 0. provides small negative values input less 0.","code":""},{"path":"neural-networks.html","id":"what-is-the-dying-relu-problem-in-neural-networks","chapter":"6 Neural Networks","heading":"6.3 What is the “dying ReLU” problem in neural networks?","text":"ReLu provides output zero input(large negative biases). problem occur due high learning rate large negative bias. Leaky ReLU commonly used method overcome dying ReLU problem. adds small negative slope prevent dying ReLU problem.","code":""},{"path":"neural-networks.html","id":"why-is-rectified-linear-unit-a-good-activation-function","chapter":"6 Neural Networks","heading":"6.4 Why is Rectified Linear Unit a good activation function?","text":"Rectified Linear Unit, also known ReLU function, known better activation function sigmoid function tanh function performs gradient descent faster. Notice image left x (z) large, slope small, slows gradient descent significantly. , however, case ReLU function.","code":""},{"path":"neural-networks.html","id":"what-is-the-activation-function-why-do-we-need-them","chapter":"6 Neural Networks","heading":"6.5 What is the activation function? Why do we need them?","text":"Activation functions mathematical functions transform output neural network certain scale. means normalizes output range 0 1 -1 1. Activation functions neural networks introduce non-linearity. helps neural networks handle non-linear relationships.","code":""},{"path":"neural-networks.html","id":"what-is-backward-and-forward-propagation","chapter":"6 Neural Networks","heading":"6.6 What is backward and forward propagation?","text":"Backpropagation traverses reverse direction. computes gradient(delta rule) parameters(weights biases) order map output layer input layer. main objective backpropagation minimize error. process repeat error minimized final parameters used producing output.Forward propagation forward pass computes intermediate values order map input output layer.","code":""},{"path":"neural-networks.html","id":"what-is-backpropagation-in-neural-networks-in-a-pure-mathematical-sense","chapter":"6 Neural Networks","heading":"6.7 What is backpropagation in neural networks in a pure mathematical sense?","text":"backpropagation Computer science algorithmic way send result computation back parent recursively.Machine learning, backpropagation sends feedback neural net.complete algorithm known Gradient Descent. part Gradient Descent(similar algorithms) infer error(usually calculus) correct known Backpropagation.training step includes calculating gradient(differentiation calculus) backpropagation(integrating gradient get back way weights change).","code":""},{"path":"neural-networks.html","id":"cost-function","chapter":"6 Neural Networks","heading":"6.8 Cost Function","text":"cost function neural network similar cost function use machine learning model. ’s measure ’good” neural network regards values predicts compared actual values. cost function inversely proportional quality model — better model, lower cost function vice versa.purpose cost function value optimize. minimizing cost function neural network, ’ll achieve optimal weights parameters model, thereby maximizing performance .\nseveral commonly used cost functions, including quadratic cost, cross-entropy cost, exponential cost, Hellinger distance, Kullback-Leibler divergence, etc.main objective neural network find optimal set weights biases minimizing cost function. Cost function loss function measure used measure performance neural network test data set. measures ability estimate relationship X y. example cost functions mean square error.","code":""},{"path":"neural-networks.html","id":"backpropagation","chapter":"6 Neural Networks","heading":"6.9 Backpropagation","text":"Backpropagation algorithm closely ties cost function. Specifically, algorithm used compute gradient cost function. adopted lot popularity use due speed & efficiency compared approaches.name stems fact calculation gradient starts gradient final layer weights moves backwards gradient first layer weights. Consequently, error layer k dependent next layer k+1.Generally, backpropagation works follows:Calculates forward phase input-output pairCalculates backward phase pairCombine individual gradientsUpdate weights based learning rate total gradient","code":""},{"path":"neural-networks.html","id":"difference-between-convex-and-non-convex-cost-function-what-does-it-mean-when-a-cost-function-is-non-convex","chapter":"6 Neural Networks","heading":"6.10 Difference between convex and non-convex cost function; what does it mean when a cost function is non-convex?","text":"convex function one line drawn two points graph lies graph. one minimum.non-convex function one line drawn two points graph may intersect points graph. characterized “wavy”.cost function non-convex, means ’s likelihood function may find local minima instead global minimum, typically undesired machine learning models optimization perspective.","code":""},{"path":"neural-networks.html","id":"convolutional-neural-networks","chapter":"6 Neural Networks","heading":"6.11 Convolutional Neural Networks","text":"","code":""},{"path":"neural-networks.html","id":"what-is-cnn-how-does-cnn-work","chapter":"6 Neural Networks","heading":"6.11.1 What is CNN? How does CNN work?","text":"CNN Feedforward neural network. CNN filters raw image detail patterns classifies using traditional neural network. Convolution focuses small patches image represents weighted sum image pixel values. offers applications Image recognition object detection. works following steps:Convolution OperationConvolution OperationReLu layerReLu layerPooling- Max Min PoolPooling- Max Min PoolFlatteningFlatteningFull connectionFull connectionA Convolutional Neural Network (CNN) type neural network takes input (usually image), assigns importance different features image, outputs prediction. makes CNNs better feedforward neural networks better captures spatial (pixel) dependencies throughout image, meaning can understand composition image better.interested, CNNs use mathematical operation called convolution. Wikipedia defines convolution mathematical operation two functions produces third function expressing shape one modified . Thus, CNN’s use convolution instead general matrix multiplication least one layers.TLDR: CNNs type neural network mainly used image classification.","code":""},{"path":"neural-networks.html","id":"what-are-convolution-layers","chapter":"6 Neural Networks","heading":"6.11.2 What are Convolution layers?","text":"convolution layer inspired visual cortex. converts image layers, transforms small images, extracts features images. sum results single output pixel. captures relationship pixels detects edge, blur, sharpen features.","code":""},{"path":"neural-networks.html","id":"recurrent-neural-networks","chapter":"6 Neural Networks","heading":"6.12 Recurrent Neural Networks","text":"Recurrent Neural Network (RNNs) another type neural network works exceptionally well sequential data due ability ingest inputs varying sizes. RNNs consider current input well previous inputs given, means input can technically produce different output based previous inputs given.Technically speaking, RNNs type neural network connections nodes form digraph along temporal sequence, allowing use internal memory process variable-length sequences inputs.TLDR: RNNs type neural network mainly used sequential time-series data.Recurrent neural networks, also known RNNs, class neural networks allow previous outputs used inputs hidden states.commonly used recognize pattern sequences data, including time-series data, stock market data, etc.","code":""},{"path":"neural-networks.html","id":"long-short-term-memory-networks","chapter":"6 Neural Networks","heading":"6.13 Long Short-Term Memory Networks","text":"Long Short-Term Memory (LSTM) networks type Recurrent Neural Networks addresses one shortfalls regular RNNs: RNNs short-term memory.\nSpecifically, sequence long, .e. lag greater 5–10 steps, RNNs tend dismiss information provided earlier steps. example, fed paragraph RNN, may overlook information provided beginning paragraph.Thus LSTMs created resolve issue.","code":""},{"path":"neural-networks.html","id":"weight-initialization","chapter":"6 Neural Networks","heading":"6.14 Weight Initialization","text":"point weight initialization make sure neural network doesn’t converge trivial solution.\nweights initialized value(eg. equal zero) unit get exactly signal every layer behave single cell.Therefore, want randomly initialize weights near zero, equal zero. expectation stochastic optimization algorithm ’s used train model.","code":""},{"path":"neural-networks.html","id":"how-are-weights-initialized-in-a-network","chapter":"6 Neural Networks","heading":"6.14.1 How are weights initialized in a Network?","text":"weights neural network MUST initialized randomly expectation stochastic gradient descent.initialized weights value (.e. zero one), hidden unit get exactly signal. example, weights initialized 0, hidden units get zero signal.","code":""},{"path":"neural-networks.html","id":"batch-vs.-stochastic-gradient-descent","chapter":"6 Neural Networks","heading":"6.15 Batch vs. Stochastic Gradient Descent","text":"Batch gradient descent stochastic gradient descent two different methods used compute gradient.Batch gradient descent simply computes gradient using whole dataset. much slower especially larger datasets better convex smooth error manifolds.\nstochastic gradient descent, gradient computed using single training sample time. , computationally faster less expensive. Consequently, however, global optimum reached, tends bounce around — results good solution optimal solution.","code":""},{"path":"neural-networks.html","id":"hyper-parameters","chapter":"6 Neural Networks","heading":"6.16 Hyper-parameters","text":"Hyper-parameters variables regulate network structure variables govern network trained. Common hyper-parameters include following:\n- Model architecture parameters number layers, number hidden units, etc.learning rate (alpha)learning rate (alpha)Network weight initializationNetwork weight initializationNumber epochs (defined one cycle whole training dataset)Number epochs (defined one cycle whole training dataset)Batch sizeBatch sizeand ..","code":""},{"path":"neural-networks.html","id":"learning-rate","chapter":"6 Neural Networks","heading":"6.17 Learning Rate","text":"learning rate hyper-parameter used neural networks control much adjust model response estimated error time model weights updated.learning rate low, model train slowly minimal updates made weights iteration. Thus, take many updates reaching minimum point.learning rate set high, causes undesirable divergent behavior loss function due drastic updates weights, may fail converge.","code":""},{"path":"neural-networks.html","id":"what-is-deep-learning","chapter":"6 Neural Networks","heading":"6.18 What is Deep Learning?","text":"Deep Learning subdomain Machine Learning. deep learning, large number layers architecture. successive layers learn complex patterns data. Deep Learning offers various applications text, voice, image, video data.","code":""},{"path":"neural-networks.html","id":"what-is-gradient-descent-how-does-it-work","chapter":"6 Neural Networks","heading":"6.19 What is Gradient Descent? How does it work?","text":"first-order iterative optimization technique finding minimum function. efficient optimization technique find local global minimum.\ngradient descent, gradient step taken point. takes current value parameters updates help gradient step width. gradient recomputed steps decremented iteration. process continues convergence achieved.Types Gradient DescentFull batch gradient descent uses full dataset.Full batch gradient descent uses full dataset.Stochastic gradient descent uses sample dataset.Stochastic gradient descent uses sample dataset.","code":""},{"path":"neural-networks.html","id":"stochastic-gradient-descent","chapter":"6 Neural Networks","heading":"6.19.1 Stochastic Gradient Descent","text":"Stochastic Gradient Descent builds top Gradient Descent can work complicated Cost Functions.","code":""},{"path":"neural-networks.html","id":"gradient-descent-stuck-in-local-minima-and-misses-true-minima.","chapter":"6 Neural Networks","heading":"6.19.2 Gradient Descent Stuck in Local Minima and Misses True Minima.","text":"Gradient Descent works well case convex cost functions one minimum . However, case complicated Cost Functions, Gradient Descent can easily get stuck local minima ruins Neural Network learning.","code":""},{"path":"neural-networks.html","id":"stochastic-vs.-gradient-descent","chapter":"6 Neural Networks","heading":"6.19.3 Stochastic vs. Gradient Descent","text":"understand Stochastic different Gradient Descent let’s take example. assume labeled data rows ’re inputting Neural Network training.","code":""},{"path":"neural-networks.html","id":"what-is-the-difference-between-model-parameters-and-hyperparameters","chapter":"6 Neural Networks","heading":"6.20 What is the difference between model parameters and hyperparameters?","text":"Model parameters internal can estimated data. Model hyperparameters external model can estimated data.","code":""},{"path":"neural-networks.html","id":"what-do-you-mean-by-dropout-and-batch-normalization","chapter":"6 Neural Networks","heading":"6.21 What do you mean by Dropout and Batch Normalization?","text":"Dropout technique normalization. drops deactivates neurons neural network remove problem overfitting. words, introduces noise neural network model capable generalize model.Normalization used reduce algorithm time spends oscillation different values. brings features input scale.Batch Normalization also normalizing values hidden states small batches data. research shown removing Dropout Batch Normalization improves learning rate without loss generalization.","code":""},{"path":"neural-networks.html","id":"what-is-vanishing-gradient-descent","chapter":"6 Neural Networks","heading":"6.22 What is vanishing gradient descent?","text":"RNN follows chain rule backpropagation. one gradients approaches zero gradient move towards zero. small value sufficient training model. , small gradient means weights biases neural network updated effectively.Also, hidden layers activation functions sigmoid function Tanh causes small derivatives decrease gradient.solution Vanishing Gradient DescentUse different activation function ReLu(Rectified Linear Unit).Use different activation function ReLu(Rectified Linear Unit).Batch normalization can also solve problem simply normalizing input space.Batch normalization can also solve problem simply normalizing input space.Weight initializationWeight initializationLSTMLSTM","code":""},{"path":"neural-networks.html","id":"what-is-exploding-gradient-descent","chapter":"6 Neural Networks","heading":"6.23 What is exploding gradient descent?","text":"Exploding gradient just opposite situation vanishing gradient. -large value RNN causes powerful training. can overcome problem using Truncated Backpropagation, penalties, Gradient Clipping.Gradient direction magnitude calculated training neural network used update network weights right direction right amount.“Exploding gradients problem large error gradients accumulate result large updates neural network model weights training.” extreme, values weights can become large overflow result NaN values.effect model unstable unable learn training data.","code":""},{"path":"neural-networks.html","id":"what-is-lstm-and-bilstm","chapter":"6 Neural Networks","heading":"6.24 What is LSTM and BiLSTM?","text":"LSTM special type RNN. also uses chain-like structure capability learn remember long-term sequences. LSTM handles issue vanishing gradient. keeps gradient step enough therefore short training high accuracy. uses gated cells write, read, erase value. three gates: input, forget, output gate.BiLSTM learns sequential long terms directions. captures information previous next states. Finally, merges results two states produces output. memorizes information sentence directions.","code":""},{"path":"neural-networks.html","id":"explain-gates-used-in-lstm-with-their-functions.","chapter":"6 Neural Networks","heading":"6.25 Explain gates used in LSTM with their functions.","text":"LSTM three gates: input, forget, output gate. input gate used add information network, forget used discard information, output gate decides information pass hidden output layer.","code":""},{"path":"neural-networks.html","id":"what-is-the-difference-between-lstm-and-gru","chapter":"6 Neural Networks","heading":"6.26 What is the difference between LSTM and GRU?","text":"GRU also type RNN. slightly different LSTM. main difference LSTM GRU Gates number gates.LSTM uses three gates(input, forget, output gate) GRU uses two gates(reset update).LSTM uses three gates(input, forget, output gate) GRU uses two gates(reset update).GRU, update gate similar input forget gate LSTM reset gate another gate used decide much past information forget.GRU, update gate similar input forget gate LSTM reset gate another gate used decide much past information forget.GRU faster compared LSTM.GRU faster compared LSTM.GRU needs fewer data generalize.GRU needs fewer data generalize.","code":""},{"path":"neural-networks.html","id":"what-is-padding","chapter":"6 Neural Networks","heading":"6.27 What is padding?","text":"Sometimes filter unable fit input image perfectly. two strategies padding: Zero padding valid padding. Zero paddings add zero image filter fits image. Valid padding drops part image. (Drop part image)","code":""},{"path":"neural-networks.html","id":"what-are-pooling-and-flattening","chapter":"6 Neural Networks","heading":"6.28 What are pooling and Flattening?","text":"Pooling used reduce spatial size selects important pixel values features. also known Downsampling. also makes faster computation reducing dimension. Pooling summarizes sub-region captures rotational positional invariant features.Pooling: several pooling operations common max pooling. teaches network spatial variance, simple words ability recognize image features even image upside , tilted, image taken far close, etc. output operation pooled feature map.Flattening: purpose operation able input pooled feature map neural network.\nimage shows entire CNN operation.","code":""},{"path":"neural-networks.html","id":"what-is-epoch-batch-and-iteration-in-deep-learning","chapter":"6 Neural Networks","heading":"6.29 What is Epoch, Batch, and Iteration in Deep Learning?","text":"Epoch one-pass entire dataset. , one pass = one forward pass + one backward passEpoch one-pass entire dataset. , one pass = one forward pass + one backward passBatch size number training examples one forward/backward pass. larger batch size requires memory space.Batch size number training examples one forward/backward pass. larger batch size requires memory space.Iteration number passes. pass using batch size number times batch data passed algorithm.Iteration number passes. pass using batch size number times batch data passed algorithm.example, 1000 training examples, batch size 200, take 5 iterations complete 1 epoch.example, 1000 training examples, batch size 200, take 5 iterations complete 1 epoch.","code":""},{"path":"neural-networks.html","id":"what-is-an-auto-encoder","chapter":"6 Neural Networks","heading":"6.30 What is an Auto-encoder?","text":"Autoencoders unsupervised deep learning techniques reduce dimension data encode. Autoencoders encoded data one side decoded another side. encoding, transforms data reduced representation called code embedding(also known latent-space representation). embedding transformed output. Autoencoders can dimensionality reduction improve performance algorithm.","code":""},{"path":"neural-networks.html","id":"what-do-you-understand-by-boltzmann-machine-and-restricted-boltzmann-machines","chapter":"6 Neural Networks","heading":"6.31 What do you understand by Boltzmann Machine and Restricted Boltzmann Machines?","text":"Boltzmann machines stochastic(non-deterministic models) generative neural networks. capability discover interesting features represent complex patterns data. Boltzmann Machine uses many layers feature detectors make slower network. Restricted Boltzmann Machines (RBMs) single layer feature detectors makes faster compared Boltzmann Machine.RBM neural network model also known Energy-Based Models. RBM offers various applications recommender systems, classification, regression, topic modeling, dimensionality reduction.","code":""},{"path":"neural-networks.html","id":"explain-generative-adversarial-network.","chapter":"6 Neural Networks","heading":"6.32 Explain Generative Adversarial Network.","text":"GAN (Generative Adversarial Network) unsupervised deep learning trains two networks time. two components: Generator Discriminator. generator generates images close real image discriminator determines difference fake real images. GAN able produce new content.","code":""},{"path":"neural-networks.html","id":"what-is-adam-whats-the-main-difference-between-adam-and-sgd","chapter":"6 Neural Networks","heading":"6.33 What is Adam? What’s the main difference between Adam and SGD?","text":"Adam (Adaptive Moment Estimation) optimization technique training neural networks. average, best optimizer .works momentums first second order. intuition behind Adam don’t want roll fast just can jump minimum, want decrease velocity little bit careful search.\nAdam tends converge faster, SGD often converges optimal solutions. SGD’s high variance disadvantages gets rectified Adam (advantage Adam).","code":""},{"path":"neural-networks.html","id":"when-would-you-use-adam-and-when-sgd","chapter":"6 Neural Networks","heading":"6.34 When would you use Adam and when SGD?","text":"Adam tends converge faster, SGD often converges optimal solutions.","code":""},{"path":"neural-networks.html","id":"neural-network-how-do-they-learn","chapter":"6 Neural Networks","heading":"6.35 Neural Network: How Do They Learn","text":"learning happens passing labeled data way input layers output layers way back. Since data labeled Neural network knows ’s expected output compares actual output Neural Network.first Epoch, labeled data entered input layer propagated output layer Neural Network calculate output. difference actual output Neural Network vs. expected output called Cost Function. goal Neural Network decrease Cost Function much possible. , Neural Network Back-Propagate output layer way input layer update weights Neurons accordingly attempt minimize Cost Function.act sending data input layer output layer way back called Epoch. epoch, Neural Network updates weights Neurons also known Learning. multiple Epochs weight updates, loss function (difference Neural Network output vs. Actual output) reach minimum.","code":""},{"path":"neural-networks.html","id":"neural-network-during-learning-phase","chapter":"6 Neural Networks","heading":"6.36 Neural Network during Learning Phase","text":"Upon learning, Neurons different weights weights dictate future outputs. example earlier car vs. bus classification scenario Neuron looking number windows decide object car bus, obviously, Neuron higher weight Neuron looking color object determine ’s car bus. oversimplification Neurons function want get idea Neuron weights according importance.","code":""},{"path":"neural-networks.html","id":"gradient-descent","chapter":"6 Neural Networks","heading":"6.37 Gradient Descent","text":"Gradient Descent method minimize Cost Function order update Neurons weights. method tells Neural Network calculate Cost Function fast efficient manner minimize difference actual expected outputs.easiest understand common example comparing Cost Function ball trying find lowest point updating slope.Gradient (Batch) Descent Neural Network goes data one row time calculate actual output row. finishing rows dataset, Neural Network compares cumulative total output rows expected output backpropagates update weights. means Neural Network updates weights working entire dataset one big batch, ’s big timely Epoch. Neural Network several times train network.Stochastic Gradient Descent Neural Network goes data one row time calculate actual output row. Right away Neural Network compares actual output first row expected output backpropagates update weights, completes Epoch. happens second row, comparing outputs backpropagating update weights. way last row, multiple Epochs multiple weight updating happen go entire dataset rather treating one big batch like case Gradient Descent. helps avoid local minimums ’s faster Gradient Descent cause doesn’t need load data memory run rather loads one row time updates weights.best worlds method ’s called mini-batch Gradient Descent basically combining . decide many rows run update . instead running whole dataset one batch instead running one row time, flexibility choose number rows run.","code":""},{"path":"neural-networks.html","id":"back-propagation","chapter":"6 Neural Networks","heading":"6.38 Back-Propagation","text":"now know back-propagation don’t ’s simply adjusting weights Neurons Neural Network calculating Cost Function. Back-Propagation Neural Network learns result calculating Cost Function. important concept know Back-Propagation updates weights Neurons simultaneously.training purposes, beginning, weights Neurons randomly initialized small numbers learning back-propagation weights start get updated meaningful values.","code":""},{"path":"neural-networks.html","id":"softmax-function","chapter":"6 Neural Networks","heading":"6.39 Softmax Function","text":"mainly used classification problems multi-class predictions. typically output layer Neural Network.apply different activation functions hidden layers output layers. diagram, ReLU applied hidden layers Sigmoid applied output layer. common predict probability something.","code":""},{"path":"neural-networks.html","id":"what-happens-if-the-learning-rate-is-set-too-high-or-too-low","chapter":"6 Neural Networks","heading":"6.40 What happens if the learning rate is set too high or too low?","text":"learning rate low, model train slowly minimal updates made weights iteration. Thus, take many updates reaching minimum point.learning rate set high, causes undesirable divergent behavior loss function due drastic updates weights, may fail converge.","code":""},{"path":"sql.html","id":"sql","chapter":"7 SQL","heading":"7 SQL","text":"finished nice book.","code":""},{"path":"references.html","id":"references","chapter":"References","heading":"References","text":"","code":""}]
