<!DOCTYPE html>
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<title>Chapter 3 Metrics | Machine Learning Prep Guide</title>
<meta name="author" content="Anjali Chauhan">
<meta name="description" content="3.1 Accuracy: (TP+TN/(TP+FP+FN+TN) WHAT: Accuracy is the proportion of true results among the total number of cases examined. WHEN: Accuracy is a valid choice of evaluation for classification...">
<meta name="generator" content="bookdown 0.26 with bs4_book()">
<meta property="og:title" content="Chapter 3 Metrics | Machine Learning Prep Guide">
<meta property="og:type" content="book">
<meta property="og:description" content="3.1 Accuracy: (TP+TN/(TP+FP+FN+TN) WHAT: Accuracy is the proportion of true results among the total number of cases examined. WHEN: Accuracy is a valid choice of evaluation for classification...">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Chapter 3 Metrics | Machine Learning Prep Guide">
<meta name="twitter:description" content="3.1 Accuracy: (TP+TN/(TP+FP+FN+TN) WHAT: Accuracy is the proportion of true results among the total number of cases examined. WHEN: Accuracy is a valid choice of evaluation for classification...">
<!-- JS --><script src="https://cdnjs.cloudflare.com/ajax/libs/clipboard.js/2.0.6/clipboard.min.js" integrity="sha256-inc5kl9MA1hkeYUt+EC3BhlIgyp/2jDIyBLS6k3UxPI=" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/fuse.js/6.4.6/fuse.js" integrity="sha512-zv6Ywkjyktsohkbp9bb45V6tEMoWhzFzXis+LrMehmJZZSys19Yxf1dopHx7WzIKxr5tK2dVcYmaCk2uqdjF4A==" crossorigin="anonymous"></script><script src="https://kit.fontawesome.com/6ecbd6c532.js" crossorigin="anonymous"></script><script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<link href="libs/bootstrap-4.6.0/bootstrap.min.css" rel="stylesheet">
<script src="libs/bootstrap-4.6.0/bootstrap.bundle.min.js"></script><script src="libs/bs3compat-0.3.1/transition.js"></script><script src="libs/bs3compat-0.3.1/tabs.js"></script><script src="libs/bs3compat-0.3.1/bs3compat.js"></script><link href="libs/bs4_book-1.0.0/bs4_book.css" rel="stylesheet">
<script src="libs/bs4_book-1.0.0/bs4_book.js"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/autocomplete.js/0.38.0/autocomplete.jquery.min.js" integrity="sha512-GU9ayf+66Xx2TmpxqJpliWbT5PiGYxpaG8rfnBEk1LL8l1KGkRShhngwdXK1UgqhAzWpZHSiYPc09/NwDQIGyg==" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/mark.js/8.11.1/mark.min.js" integrity="sha512-5CYOlHXGh6QpOFA/TeTylKLWfB3ftPsde7AnmhuitiTX4K5SqCLBeKro6sPS8ilsz1Q4NRx3v8Ko2IBiszzdww==" crossorigin="anonymous"></script><!-- CSS --><style type="text/css">
    
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
  </style>
</head>
<body data-spy="scroll" data-target="#toc">

<div class="container-fluid">
<div class="row">
  <header class="col-sm-12 col-lg-3 sidebar sidebar-book"><a class="sr-only sr-only-focusable" href="#content">Skip to main content</a>

    <div class="d-flex align-items-start justify-content-between">
      <h1>
        <a href="index.html" title="">Machine Learning Prep Guide</a>
      </h1>
      <button class="btn btn-outline-primary d-lg-none ml-2 mt-1" type="button" data-toggle="collapse" data-target="#main-nav" aria-expanded="true" aria-controls="main-nav"><i class="fas fa-bars"></i><span class="sr-only">Show table of contents</span></button>
    </div>

    <div id="main-nav" class="collapse-lg">
      <form role="search">
        <input id="search" class="form-control" type="search" placeholder="Search" aria-label="Search">
</form>

      <nav aria-label="Table of contents"><h2>Table of contents</h2>
        <ul class="book-toc list-unstyled">
<li><a class="" href="index.html"><span class="header-section-number">1</span> Prerequisites</a></li>
<li><a class="" href="intro.html"><span class="header-section-number">2</span> ML Models</a></li>
<li><a class="active" href="metrics.html"><span class="header-section-number">3</span> Metrics</a></li>
<li><a class="" href="clustering.html"><span class="header-section-number">4</span> Clustering</a></li>
<li><a class="" href="data-preprocessing.html"><span class="header-section-number">5</span> Data Preprocessing</a></li>
<li><a class="" href="neural-networks.html"><span class="header-section-number">6</span> Neural Networks</a></li>
<li><a class="" href="sql.html"><span class="header-section-number">7</span> SQL</a></li>
<li><a class="" href="references.html">References</a></li>
</ul>

        <div class="book-extra">
          
        </div>
      </nav>
</div>
  </header><main class="col-sm-12 col-md-9 col-lg-7" id="content"><div id="metrics" class="section level1" number="3">
<h1>
<span class="header-section-number">3</span> Metrics<a class="anchor" aria-label="anchor" href="#metrics"><i class="fas fa-link"></i></a>
</h1>
<div id="accuracy" class="section level2" number="3.1">
<h2>
<span class="header-section-number">3.1</span> Accuracy:<a class="anchor" aria-label="anchor" href="#accuracy"><i class="fas fa-link"></i></a>
</h2>
<p><strong><em>(TP+TN/(TP+FP+FN+TN)</em></strong></p>
<ul>
<li><p><strong>WHAT</strong>: Accuracy is the proportion of true results among the total number of cases examined.</p></li>
<li><p><strong>WHEN</strong>: Accuracy is a valid choice of evaluation for classification problems which are well balanced and not skewed or No class imbalance.</p></li>
<li><p><strong>AWARE</strong>: What if we are predicting if an asteroid will hit the earth? Just say No all the time. And you will be 99% accurate. My model can be reasonably accurate, but not at all valuable</p></li>
</ul>
</div>
<div id="precision" class="section level2" number="3.2">
<h2>
<span class="header-section-number">3.2</span> Precision:<a class="anchor" aria-label="anchor" href="#precision"><i class="fas fa-link"></i></a>
</h2>
<p><strong><em>(TP)/(TP+FP)</em></strong></p>
<ul>
<li>
<p><strong>WHAT</strong>: It answers the question: what proportion of predicted Positives is truly Positive?</p>
<ul>
<li>Asteroid problem, we never predicted a true positive, precision = 0</li>
</ul>
</li>
<li><p><strong>WHEN</strong>: Precision is a valid choice of evaluation metric when we want to be very sure of our prediction. For example: If we are building a system to predict if we should decrease the credit limit on a particular account, we want to be very sure about our prediction or it may result in customer dissatisfaction.</p></li>
<li><p><strong>AWARE</strong>: Being very precise means our model will leave a lot of credit defaulters untouched and hence lose money.</p></li>
</ul>
</div>
<div id="recall" class="section level2" number="3.3">
<h2>
<span class="header-section-number">3.3</span> Recall:<a class="anchor" aria-label="anchor" href="#recall"><i class="fas fa-link"></i></a>
</h2>
<p><strong><em>(TP)/(TP+FN)</em></strong></p>
<ul>
<li>
<p><strong>WHAT</strong>: answers a different question: what proportion of actual Positives is correctly classified?</p>
<ul>
<li>Asteroid problem, we never predicted a true positive, recall = 0</li>
</ul>
</li>
<li><p><strong>WHEN</strong>: Recall is a valid choice of evaluation metric when we want to capture as many positives as possible. For example: If we are building a system to predict if a person has cancer or not, we want to capture the disease even if we are not very sure.</p></li>
<li><p><strong>AWARE</strong>: Recall is 1 if we predict 1 for all examples.</p></li>
</ul>
</div>
<div id="f1-score" class="section level2" number="3.4">
<h2>
<span class="header-section-number">3.4</span> F1 Score:<a class="anchor" aria-label="anchor" href="#f1-score"><i class="fas fa-link"></i></a>
</h2>
<p><strong><em>2</em> (precision<em>recall)/(precision + recall)</em></strong></p>
<ul>
<li>
<p><strong>WHAT</strong>: Here we utilize the tradeoff of precision vs. recall. The F1 score is a number between 0 and 1 and is the harmonic mean of precision and recall.</p>
<ul>
<li><p>The F1 score is a number between 0 and 1 and is the harmonic mean of precision and recall.</p></li>
<li><p>We are predicting if an asteroid will hit the earth or not.</p></li>
<li><p>So if we say “No” for the whole training set. Our precision here is 0. What is the recall of our positive class? It is zero. What is the accuracy? It is more than 99%.</p></li>
<li><p>And hence the F1 score is also 0. And thus we get to know that the classifier that has an accuracy of 99% is basically worthless for our case. And hence it solves our problem.</p></li>
</ul>
</li>
<li><p><strong>WHEN</strong>: We want to have a model with both good precision and recall.</p></li>
<li><p>F1 score sort of maintains a balance between the precision and recall for your classifier. If your precision is low, the F1 is low and if the recall is low again your F1 score is low.</p></li>
<li><p><strong>EXAMPLE</strong>: If you are a police inspector and you want to catch criminals, you want to be sure that the person you catch is a criminal (Precision) and you also want to capture as many criminals (Recall) as possible. The F1 score manages this tradeoff.</p></li>
<li>
<p><strong>AWARE</strong>: The main problem with the F1 score is that it gives equal weight to precision and recall. We might sometimes need to include domain knowledge in our evaluation where we want to have more recall or more precision.To solve this, we can do this by creating a weighted F1 metric as below where beta manages the tradeoff between precision and recall.</p>
<ul>
<li>ROC stands for Receiver Operating Characteristics. The diagrammatic representation that shows the contrast between true positive rate vs false positive rate. It is used when we need to predict the probability of the binary outcome. AUC is area under ROC curve. It represents degree of separability. It is used to value how much model is capable of distinguishing between classes. Higher value the better (0,1)</li>
</ul>
</li>
</ul>
</div>
<div id="which-metrics-to-use-when-classification" class="section level2" number="3.5">
<h2>
<span class="header-section-number">3.5</span> Which metrics to use when (classification)?<a class="anchor" aria-label="anchor" href="#which-metrics-to-use-when-classification"><i class="fas fa-link"></i></a>
</h2>
<p>Accuracy, Precision, and Recall:</p>
<div id="accuracy-1" class="section level3" number="3.5.1">
<h3>
<span class="header-section-number">3.5.1</span> Accuracy<a class="anchor" aria-label="anchor" href="#accuracy-1"><i class="fas fa-link"></i></a>
</h3>
<p>Accuracy is the quintessential classification metric. It is pretty easy to understand. And easily suited for binary as well as a multiclass classification problem.</p>
<p><strong>Accuracy</strong> = (TP+TN)/(TP+FP+FN+TN)</p>
<p>Accuracy is the proportion of true results among the total number of cases examined.</p>
<p><strong>When to use?</strong></p>
<p>Accuracy is a valid choice of evaluation for classification problems which are well balanced and not skewed or No class imbalance.</p>
<p><strong>Caveats</strong></p>
<p>Let us say that our target class is very sparse. Do we want accuracy as a metric of our model performance? What if we are predicting if an asteroid will hit the earth? Just say No all the time. And you will be 99% accurate. My model can be reasonably accurate, but not at all valuable.</p>
</div>
<div id="precision-1" class="section level3" number="3.5.2">
<h3>
<span class="header-section-number">3.5.2</span> Precision<a class="anchor" aria-label="anchor" href="#precision-1"><i class="fas fa-link"></i></a>
</h3>
<p>Let’s start with precision, which answers the following question: what proportion of predicted Positives is truly Positive?</p>
<p><strong>Precision</strong> = (TP)/(TP+FP)</p>
<p>In the asteroid prediction problem, we never predicted a true positive.</p>
<p>And thus precision=0</p>
<p><strong>When to use?</strong></p>
<p>Precision is a valid choice of evaluation metric when we want to be very sure of our prediction. For example: If we are building a system to predict if we should decrease the credit limit on a particular account, we want to be very sure about our prediction or it may result in customer dissatisfaction.</p>
<p><strong>Caveats</strong></p>
<p>Being very precise means our model will leave a lot of credit defaulters untouched and hence lose money.</p>
</div>
<div id="recall-1" class="section level3" number="3.5.3">
<h3>
<span class="header-section-number">3.5.3</span> Recall<a class="anchor" aria-label="anchor" href="#recall-1"><i class="fas fa-link"></i></a>
</h3>
<p>Another very useful measure is recall, which answers a different question: what proportion of actual Positives is correctly classified?</p>
<p><strong>Recall</strong> = (TP)/(TP+FN)</p>
<p>In the asteroid prediction problem, we never predicted a true positive.</p>
<p>And thus recall is also equal to 0.</p>
<p><strong>When to use?</strong></p>
<p>Recall is a valid choice of evaluation metric when we want to capture as many positives as possible. For example: If we are building a system to predict if a person has cancer or not, we want to capture the disease even if we are not very sure.</p>
<p><strong>Caveats</strong></p>
<p>Recall is 1 if we predict 1 for all examples.
And thus comes the idea of utilizing tradeoff of precision vs. recall — F1 Score.</p>
</div>
<div id="f1-score-1" class="section level3" number="3.5.4">
<h3>
<span class="header-section-number">3.5.4</span> F1 Score<a class="anchor" aria-label="anchor" href="#f1-score-1"><i class="fas fa-link"></i></a>
</h3>
<p>This is my favorite evaluation metric and I tend to use this a lot in my classification projects.
The F1 score is a number between 0 and 1 and is the harmonic mean of precision and recall.</p>
<p>Let us start with a binary prediction problem. We are predicting if an asteroid will hit the earth or not.
So if we say “No” for the whole training set. Our precision here is 0. What is the recall of our positive class? It is zero. What is the accuracy? It is more than 99%.
And hence the F1 score is also 0. And thus we get to know that the classifier that has an accuracy of 99% is basically worthless for our case. And hence it solves our problem.</p>
<p><strong>When to use?</strong></p>
<p>We want to have a model with both good precision and recall.</p>
</div>
<div id="precision-recall-tradeoff" class="section level3" number="3.5.5">
<h3>
<span class="header-section-number">3.5.5</span> Precision-Recall Tradeoff<a class="anchor" aria-label="anchor" href="#precision-recall-tradeoff"><i class="fas fa-link"></i></a>
</h3>
<p>Simply stated the F1 score sort of maintains a balance between the precision and recall for your classifier. If your precision is low, the F1 is low and if the recall is low again your F1 score is low.</p>
<p>If you are a police inspector and you want to catch criminals, you want to be sure that the person you catch is a criminal (Precision) and you also want to capture as many criminals (Recall) as possible. The F1 score manages this tradeoff.</p>
<p><strong>How to Use?</strong></p>
<p>You can calculate the F1 score for binary prediction problems using:</p>
<div class="sourceCode" id="cb2"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="co"># from sklearn.metrics import f1_score</span>

<span class="co"># y_true = [0, 1, 1, 0, 1, 1]</span>
<span class="co"># y_pred = [0, 0, 1, 0, 0, 1]</span>
<span class="co"># f1_score(y_true, y_pred)  </span></code></pre></div>
<p>This is one of my functions which I use to get the best threshold for maximizing F1 score for binary predictions. The below function iterates through possible threshold values to find the one that gives the best F1 score.</p>
<div class="sourceCode" id="cb3"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="co">## y_pred is an array of predictions</span>
<span class="co"># def bestThresshold(y_true,y_pred):</span>
<span class="co">#    best_thresh = None</span>
<span class="co">#    best_score = 0</span>
<span class="co">#    for thresh in np.arange(0.1, 0.501, 0.01):</span>
<span class="co">#        score = f1_score(y_true, np.array(y_pred)&gt;thresh)</span>
<span class="co">#        if score &gt; best_score:</span>
<span class="co">#            best_thresh = thresh</span>
<span class="co">#            best_score = score</span>
<span class="co">#    return best_score , best_thresh</span></code></pre></div>
<p><strong>Caveats</strong></p>
<p>The main problem with the F1 score is that it gives equal weight to precision and recall. We might sometimes need to include domain knowledge in our evaluation where we want to have more recall or more precision.</p>
<p>To solve this, we can do this by creating a weighted F1 metric as below where beta manages the tradeoff between precision and recall.</p>
<p>Here we give <span class="math inline">\(\beta\)</span> times as much importance to recall as precision.</p>
<div class="sourceCode" id="cb4"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="co"># from sklearn.metrics import fbeta_score</span>
 
<span class="co"># y_true = [0, 1, 1, 0, 1, 1]</span>
<span class="co"># y_pred = [0, 0, 1, 0, 0, 1]</span>
<span class="co"># fbeta_score(y_true, y_pred,beta=0.5)</span></code></pre></div>
<p>F1 Score can also be used for Multiclass problems.</p>
</div>
<div id="log-lossbinary-crossentropy" class="section level3" number="3.5.6">
<h3>
<span class="header-section-number">3.5.6</span> Log Loss/Binary Crossentropy<a class="anchor" aria-label="anchor" href="#log-lossbinary-crossentropy"><i class="fas fa-link"></i></a>
</h3>
<p>Log loss is a pretty good evaluation metric for binary classifiers and it is sometimes the optimization objective as well in case of Logistic regression and Neural Networks.
Binary Log loss for an example is given by the below formula where p is the probability of predicting 1.</p>
<p>As you can see the log loss decreases as we are fairly certain in our prediction of 1 and the true label is 1.</p>
<p><strong>When to Use?</strong></p>
<p>When the output of a classifier is prediction probabilities. Log Loss takes into account the uncertainty of your prediction based on how much it varies from the actual label. This gives us a more nuanced view of the performance of our model. In general, minimizing Log Loss gives greater accuracy for the classifier.</p>
<p><strong>How to Use?</strong></p>
<div class="sourceCode" id="cb5"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="co"># from sklearn.metrics import log_loss  </span>
<span class="co">## where y_pred are probabilities and y_true are binary class labels</span>
<span class="co"># log_loss(y_true, y_pred, eps=1e-15)</span></code></pre></div>
<p><strong>Caveats</strong></p>
<p>It is susceptible in case of imbalanced datasets. You might have to introduce class weights to penalize minority errors more or you may use this after balancing your dataset.</p>
</div>
<div id="categorical-cross-entropy" class="section level3" number="3.5.7">
<h3>
<span class="header-section-number">3.5.7</span> Categorical Cross Entropy<a class="anchor" aria-label="anchor" href="#categorical-cross-entropy"><i class="fas fa-link"></i></a>
</h3>
<p>The log loss also generalizes to the multiclass problem. The classifier in a multiclass setting must assign a probability to each class for all examples. If there are N samples belonging to M classes, then the Categorical Crossentropy is the summation of -ylogp values:</p>
<ul>
<li><p>y_ij is 1 if the sample i belongs to class j else 0</p></li>
<li><p>p_ij is the probability our classifier predicts of sample i belonging to class j.</p></li>
</ul>
<p><strong>When to Use?</strong></p>
<p>When the output of a classifier is multiclass prediction probabilities. We generally use Categorical Crossentropy in case of Neural Nets. In general, minimizing Categorical cross-entropy gives greater accuracy for the classifier.</p>
<p><strong>How to Use?</strong></p>
<div class="sourceCode" id="cb6"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="co"># from sklearn.metrics import log_loss  </span>
<span class="co">## Where y_pred is a matrix of probabilities with shape = (n_samples, n_classes) and y_true is an array of class labels</span>
<span class="co"># log_loss(y_true, y_pred, eps=1e-15)</span></code></pre></div>
<p><strong>Caveats</strong>:</p>
<p>It is susceptible in case of imbalanced datasets.</p>
</div>
<div id="auc" class="section level3" number="3.5.8">
<h3>
<span class="header-section-number">3.5.8</span> AUC<a class="anchor" aria-label="anchor" href="#auc"><i class="fas fa-link"></i></a>
</h3>
<p>AUC is the area under the ROC curve.</p>
<p>AUC ROC indicates how well the probabilities from the positive classes are separated from the negative classes</p>
</div>
<div id="roc-curve" class="section level3" number="3.5.9">
<h3>
<span class="header-section-number">3.5.9</span> ROC curve<a class="anchor" aria-label="anchor" href="#roc-curve"><i class="fas fa-link"></i></a>
</h3>
<p>We have got the probabilities from our classifier. We can use various threshold values to plot our sensitivity(TPR) and (1-specificity)(FPR) on the cure and we will have a ROC curve; where True positive rate or TPR is just the proportion of trues we are capturing using our algorithm.</p>
<p>Sensitivity = TPR(True Positive Rate)</p>
<p>Recall = TP/(TP+FN)</p>
<p>and False positive rate or FPR is just the proportion of false we are capturing using our algorithm.</p>
<p>1- Specificity = FPR(False Positive Rate)= FP/(TN+FP)</p>
<p>Here we can use the ROC curves to decide on a Threshold value.</p>
<p>The choice of threshold value will also depend on how the classifier is intended to be used.</p>
<p>If it is a cancer classification application you don’t want your threshold to be as big as 0.5. Even if a patient has a 0.3 probability of having cancer you would classify him to be 1.</p>
<p>Otherwise, in an application for reducing the limits on the credit card, you don’t want your threshold to be as less as 0.5. You are here a little worried about the negative effect of decreasing limits on customer satisfaction.</p>
<p><strong>When to Use?</strong></p>
<p>AUC is scale-invariant. It measures how well predictions are ranked, rather than their absolute values. So, for example, if you as a marketer want to find a list of users who will respond to a marketing campaign. AUC is a good metric to use since the predictions ranked by probability is the order in which you will create a list of users to send the marketing campaign.</p>
<p>Another benefit of using AUC is that it is classification-threshold-invariant like log loss. It measures the quality of the model’s predictions irrespective of what classification threshold is chosen, unlike F1 score or accuracy which depend on the choice of threshold.</p>
<p><strong>How to Use?</strong></p>
<div class="sourceCode" id="cb7"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="co"># import numpy as np</span>
<span class="co"># from sklearn.metrics import roc_auc_score</span>
<span class="co"># y_true = np.array([0, 0, 1, 1])</span>
<span class="co"># y_scores = np.array([0.1, 0.4, 0.35, 0.8])</span>
<span class="co"># print(roc_auc_score(y_true, y_scores))</span></code></pre></div>
<p><strong>Caveats</strong></p>
<p>Sometimes we will need well-calibrated probability outputs from our models and AUC doesn’t help with that.</p>
</div>
</div>
<div id="what-is-the-difference-between-precision-and-recall" class="section level2" number="3.6">
<h2>
<span class="header-section-number">3.6</span> What is the difference between precision and recall?<a class="anchor" aria-label="anchor" href="#what-is-the-difference-between-precision-and-recall"><i class="fas fa-link"></i></a>
</h2>
<p>Recall attempts to answer “What proportion of actual positives was identified correctly?”</p>
<p>Precision attempts to answer “What proportion of positive identifications was actually correct?”</p>
</div>
<div id="precision-recall-trade-off" class="section level2" number="3.7">
<h2>
<span class="header-section-number">3.7</span> Precision-recall trade-off<a class="anchor" aria-label="anchor" href="#precision-recall-trade-off"><i class="fas fa-link"></i></a>
</h2>
<p>Tradeoff means increasing one parameter would lead to decreasing of other. Precision-recall tradeoff occur due to increasing one of the parameter(precision or recall) while keeping the model same.</p>
<p>In an ideal scenario where there is a perfectly separable data, both precision and recall can get maximum value of 1.0. But in most of the practical situations, there is noise in the dataset and the dataset is not perfectly separable. There might be some points of positive class closer to the negative class and vice versa. In such cases, shifting the decision boundary can either increase the precision or recall but not both. Increasing one parameter leads to decreasing of the other.</p>
</div>
<div id="what-is-the-roc-curve-when-to-use-it" class="section level2" number="3.8">
<h2>
<span class="header-section-number">3.8</span> What is the ROC curve? When to use it?<a class="anchor" aria-label="anchor" href="#what-is-the-roc-curve-when-to-use-it"><i class="fas fa-link"></i></a>
</h2>
<p>ROC stands for Receiver Operating Characteristics. The diagrammatic representation that shows the contrast between true positive rate vs false positive rate. It is used when we need to predict the probability of the binary outcome.</p>
</div>
<div id="what-is-auc-au-roc-when-to-use-it" class="section level2" number="3.9">
<h2>
<span class="header-section-number">3.9</span> What is AUC (AU ROC)? When to use it?<a class="anchor" aria-label="anchor" href="#what-is-auc-au-roc-when-to-use-it"><i class="fas fa-link"></i></a>
</h2>
<p>AUC stands for Area Under the ROC Curve. ROC is a probability curve and AUC represents degree or measure of separability. It’s used when we need to value how much model is capable of distinguishing between classes. The value is between 0 and 1, the higher the better.</p>
</div>
<div id="we-have-two-models-one-with-85-accuracy-one-82.-which-one-do-you-pick" class="section level2" number="3.10">
<h2>
<span class="header-section-number">3.10</span> We have two models, one with 85% accuracy, one 82%. Which one do you pick?<a class="anchor" aria-label="anchor" href="#we-have-two-models-one-with-85-accuracy-one-82.-which-one-do-you-pick"><i class="fas fa-link"></i></a>
</h2>
<p>If we only care about the accuracy of the model then we would choose the one with 85%. But if an interviewer were to ask this, it would probably be a good idea to get more context, i.e. what the model is trying to predict. This will give us a better idea whether the evaluation metric should indeed be accuracy or another metric like recall or f1 score.</p>
</div>
<div id="is-accuracy-always-a-good-metric" class="section level2" number="3.11">
<h2>
<span class="header-section-number">3.11</span> Is accuracy always a good metric?<a class="anchor" aria-label="anchor" href="#is-accuracy-always-a-good-metric"><i class="fas fa-link"></i></a>
</h2>
<p>Accuracy is not a good performance metric when there is imbalance in the dataset. For example, in binary classification with 95% of A class and 5% of B class, a constant prediction of A class would have an accuracy of 95%. In case of imbalance dataset, we need to choose Precision, recall, or F1 Score depending on the problem we are trying to solve.</p>
</div>
<div id="what-is-a-confusion-matrix" class="section level2" number="3.12">
<h2>
<span class="header-section-number">3.12</span> What is a confusion matrix?<a class="anchor" aria-label="anchor" href="#what-is-a-confusion-matrix"><i class="fas fa-link"></i></a>
</h2>
<p>A confusion matrix, also known as an error matrix, is a summarized table used to assess the performance of a classification model. The number of correct and incorrect predictions are summarized with count values and broken down by each class.</p>
<ul>
<li><p><strong>True positive(TP)</strong> — Correct positive prediction</p></li>
<li><p><strong>False positive(FP)</strong> — Incorrect positive prediction</p></li>
<li><p><strong>True negative(TN)</strong> — Correct negative prediction</p></li>
<li><p><strong>False negative(FN)</strong> — Incorrect negative prediction</p></li>
</ul>
</div>
<div id="how-to-check-if-the-regression-model-fits-the-data-well" class="section level2" number="3.13">
<h2>
<span class="header-section-number">3.13</span> How to check if the regression model fits the data well?<a class="anchor" aria-label="anchor" href="#how-to-check-if-the-regression-model-fits-the-data-well"><i class="fas fa-link"></i></a>
</h2>
<p>There are a couple of metrics that you can use:</p>
<ul>
<li><p><strong>R-squared/Adjusted R-squared</strong>: Relative measure of fit. This was explained in a previous answer</p></li>
<li><p><strong>F1 Score</strong>: Evaluates the null hypothesis that all regression coefficients are equal to zero vs the alternative hypothesis that at least one doesn’t equal zero</p></li>
<li><p><strong>RMSE</strong>: Absolute measure of fit.</p></li>
</ul>
</div>
<div id="model-evaluation" class="section level2" number="3.14">
<h2>
<span class="header-section-number">3.14</span> Model Evaluation<a class="anchor" aria-label="anchor" href="#model-evaluation"><i class="fas fa-link"></i></a>
</h2>
<p>To evaluate a regression model, you can calculate its R-squared, which tells us how much of the variability in the data that the model accounts for. For example, if a model has an R-squared of 80%, then 80% of the variation in the data can be explained by the model.</p>
<p>The adjusted R-squared is a modified version of r-squared that adjusts for the number of predictors in the model; it increases if the new term improves the model more than would be expected by chance and vice versa.</p>

</div>
</div>
  <div class="chapter-nav">
<div class="prev"><a href="intro.html"><span class="header-section-number">2</span> ML Models</a></div>
<div class="next"><a href="clustering.html"><span class="header-section-number">4</span> Clustering</a></div>
</div></main><div class="col-md-3 col-lg-2 d-none d-md-block sidebar sidebar-chapter">
    <nav id="toc" data-toggle="toc" aria-label="On this page"><h2>On this page</h2>
      <ul class="nav navbar-nav">
<li><a class="nav-link" href="#metrics"><span class="header-section-number">3</span> Metrics</a></li>
<li><a class="nav-link" href="#accuracy"><span class="header-section-number">3.1</span> Accuracy:</a></li>
<li><a class="nav-link" href="#precision"><span class="header-section-number">3.2</span> Precision:</a></li>
<li><a class="nav-link" href="#recall"><span class="header-section-number">3.3</span> Recall:</a></li>
<li><a class="nav-link" href="#f1-score"><span class="header-section-number">3.4</span> F1 Score:</a></li>
<li>
<a class="nav-link" href="#which-metrics-to-use-when-classification"><span class="header-section-number">3.5</span> Which metrics to use when (classification)?</a><ul class="nav navbar-nav">
<li><a class="nav-link" href="#accuracy-1"><span class="header-section-number">3.5.1</span> Accuracy</a></li>
<li><a class="nav-link" href="#precision-1"><span class="header-section-number">3.5.2</span> Precision</a></li>
<li><a class="nav-link" href="#recall-1"><span class="header-section-number">3.5.3</span> Recall</a></li>
<li><a class="nav-link" href="#f1-score-1"><span class="header-section-number">3.5.4</span> F1 Score</a></li>
<li><a class="nav-link" href="#precision-recall-tradeoff"><span class="header-section-number">3.5.5</span> Precision-Recall Tradeoff</a></li>
<li><a class="nav-link" href="#log-lossbinary-crossentropy"><span class="header-section-number">3.5.6</span> Log Loss/Binary Crossentropy</a></li>
<li><a class="nav-link" href="#categorical-cross-entropy"><span class="header-section-number">3.5.7</span> Categorical Cross Entropy</a></li>
<li><a class="nav-link" href="#auc"><span class="header-section-number">3.5.8</span> AUC</a></li>
<li><a class="nav-link" href="#roc-curve"><span class="header-section-number">3.5.9</span> ROC curve</a></li>
</ul>
</li>
<li><a class="nav-link" href="#what-is-the-difference-between-precision-and-recall"><span class="header-section-number">3.6</span> What is the difference between precision and recall?</a></li>
<li><a class="nav-link" href="#precision-recall-trade-off"><span class="header-section-number">3.7</span> Precision-recall trade-off</a></li>
<li><a class="nav-link" href="#what-is-the-roc-curve-when-to-use-it"><span class="header-section-number">3.8</span> What is the ROC curve? When to use it?</a></li>
<li><a class="nav-link" href="#what-is-auc-au-roc-when-to-use-it"><span class="header-section-number">3.9</span> What is AUC (AU ROC)? When to use it?</a></li>
<li><a class="nav-link" href="#we-have-two-models-one-with-85-accuracy-one-82.-which-one-do-you-pick"><span class="header-section-number">3.10</span> We have two models, one with 85% accuracy, one 82%. Which one do you pick?</a></li>
<li><a class="nav-link" href="#is-accuracy-always-a-good-metric"><span class="header-section-number">3.11</span> Is accuracy always a good metric?</a></li>
<li><a class="nav-link" href="#what-is-a-confusion-matrix"><span class="header-section-number">3.12</span> What is a confusion matrix?</a></li>
<li><a class="nav-link" href="#how-to-check-if-the-regression-model-fits-the-data-well"><span class="header-section-number">3.13</span> How to check if the regression model fits the data well?</a></li>
<li><a class="nav-link" href="#model-evaluation"><span class="header-section-number">3.14</span> Model Evaluation</a></li>
</ul>

      <div class="book-extra">
        <ul class="list-unstyled">
          
        </ul>
</div>
    </nav>
</div>

</div>
</div> <!-- .container -->

<footer class="bg-primary text-light mt-5"><div class="container"><div class="row">

  <div class="col-12 col-md-6 mt-3">
    <p>"<strong>Machine Learning Prep Guide</strong>" was written by Anjali Chauhan. It was last built on 2022-04-27.</p>
  </div>

  <div class="col-12 col-md-6 mt-3">
    <p>This book was built by the <a class="text-light" href="https://bookdown.org">bookdown</a> R package.</p>
  </div>

</div></div>
</footer><!-- dynamically load mathjax for compatibility with self-contained --><script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script><script type="text/x-mathjax-config">const popovers = document.querySelectorAll('a.footnote-ref[data-toggle="popover"]');
for (let popover of popovers) {
  const div = document.createElement('div');
  div.setAttribute('style', 'position: absolute; top: 0, left:0; width:0, height:0, overflow: hidden; visibility: hidden;');
  div.innerHTML = popover.getAttribute('data-content');

  var has_math = div.querySelector("span.math");
  if (has_math) {
    document.body.appendChild(div);
    MathJax.Hub.Queue(["Typeset", MathJax.Hub, div]);
    MathJax.Hub.Queue(function() {
      popover.setAttribute('data-content', div.innerHTML);
      document.body.removeChild(div);
    })
  }
}
</script>
</body>
</html>
