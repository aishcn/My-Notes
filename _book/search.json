[{"path":"index.html","id":"prerequisites","chapter":"1 Prerequisites","heading":"1 Prerequisites","text":"","code":"\ninstall.packages(\"bookdown\")\n# or the development version\n# devtools::install_github(\"rstudio/bookdown\")"},{"path":"intro.html","id":"intro","chapter":"2 ML Models","heading":"2 ML Models","text":"","code":""},{"path":"intro.html","id":"linear-regression","chapter":"2 ML Models","heading":"2.1 Linear Regression","text":"Linear Regression involves finding ‘line best fit’ represents dataset using least squares method. least squares method involves finding linear equation minimizes sum squared residuals. residual equal actual minus predicted value.give example, red line better line best fit green line closer points, thus, residuals smaller.Linear Regression one fundamental algorithms used model relationships dependent variable one independent variables. simpler terms, involves finding ‘line best fit’ represents two variables.line best fit found minimizing squared distances points line best fit — known minimizing sum squared residuals. residual simply equal predicted value minus actual value.","code":""},{"path":"intro.html","id":"whats-the-difference-between-linear-regression-and-logistic-regression","chapter":"2 ML Models","heading":"2.2 What’s the difference between Linear Regression and Logistic Regression?","text":"Linear Regression used predict continuous variable mainly used solve regression problems. Linear regression finds best fit line output numerical value can predicted.Logistic Regression used predict categorical values mainly used classification problems. Logistic regression produces S curve classifies, output binary categories.","code":""},{"path":"intro.html","id":"what-is-overfitting","chapter":"2 ML Models","heading":"2.3 What is overfitting?","text":"Overfitting error model ‘fits’ data well, resulting model high variance low bias. consequence, overfit model inaccurately predict new data points even though high accuracy training data.Overfitting modeling error function fits data closely, resulting high levels error new data introduced model.number ways can prevent overfitting model:Cross-validation: Cross-validation technique used assess well model performs new independent dataset. simplest example cross-validation split data two groups: training data testing data, use training data build model testing data test model.Cross-validation: Cross-validation technique used assess well model performs new independent dataset. simplest example cross-validation split data two groups: training data testing data, use training data build model testing data test model.Regularization: Overfitting occurs models higher degree polynomials. Thus, regularization reduces overfitting penalizing higher degree polynomials.Regularization: Overfitting occurs models higher degree polynomials. Thus, regularization reduces overfitting penalizing higher degree polynomials.Reduce number features: can also reduce overfitting simply reducing number input features. can manually removing features, can use technique, called Principal Component Analysis, projects higher dimensional data (eg. 3 dimensions) smaller space (eg. 2 dimensions).Reduce number features: can also reduce overfitting simply reducing number input features. can manually removing features, can use technique, called Principal Component Analysis, projects higher dimensional data (eg. 3 dimensions) smaller space (eg. 2 dimensions).Ensemble Learning Techniques: Ensemble techniques take many weak learners converts strong learner bagging boosting. bagging boosting, techniques tend overfit less alternative counterparts.Ensemble Learning Techniques: Ensemble techniques take many weak learners converts strong learner bagging boosting. bagging boosting, techniques tend overfit less alternative counterparts.Overfitting error model ‘fits’ data well, resulting model high variance low bias. consequence, overfit model inaccurately predict new data points even though high accuracy training data.","code":""},{"path":"intro.html","id":"what-is-the-bias-variance-tradeoff","chapter":"2 ML Models","heading":"2.4 What is the bias-variance tradeoff?","text":"bias estimator difference expected value true value. model high bias tends oversimplified results underfitting. Variance represents model’s sensitivity data noise. model high variance results overfitting.Therefore, bias-variance tradeoff property machine learning models lower variance results higher bias vice versa. Generally, optimal balance two can found error minimized.","code":""},{"path":"intro.html","id":"what-are-ridge-and-lasso-regression-and-what-are-the-differences-between-them","chapter":"2 ML Models","heading":"2.5 What are ridge and lasso regression and what are the differences between them?","text":"L1 L2 regularization methods used reduce overfitting training data. Least Squares minimizes sum squared residuals, can result low bias high variance.L2 Regularization, also called ridge regression, minimizes sum squared residuals plus lambda times slope squared. additional term called Ridge Regression Penalty. increases bias model, making fit worse training data, also decreases variance.take ridge regression penalty replace absolute value slope, get Lasso regression L1 regularization.L2 less robust stable solution always one solution. L1 robust unstable solution can possibly multiple solutions.","code":""},{"path":"intro.html","id":"whats-the-difference-between-l2-and-l1-regularization","chapter":"2 ML Models","heading":"2.6 What’s the difference between L2 and L1 regularization?","text":"Penalty terms: L1 regularization uses sum absolute values weights, L2 regularization uses sum weights squared.Penalty terms: L1 regularization uses sum absolute values weights, L2 regularization uses sum weights squared.Feature selection: L1 performs feature selection reducing coefficients predictors 0, L2 .\nComputational efficiency: L2 analytical solution, L1 .Feature selection: L1 performs feature selection reducing coefficients predictors 0, L2 .\nComputational efficiency: L2 analytical solution, L1 .Multicollinearity: L2 addresses multicollinearity constraining coefficient norm.Multicollinearity: L2 addresses multicollinearity constraining coefficient norm.L1 effectively removes features unimportant, aggressively can lead underfitting. L2 weighs feature instead removing entirely, can lead better accuracy. Briefly, L1 removes features L2 doesn’t, L2 regulates weights instead.L1 effectively removes features unimportant, aggressively can lead underfitting. L2 weighs feature instead removing entirely, can lead better accuracy. Briefly, L1 removes features L2 doesn’t, L2 regulates weights instead.","code":""},{"path":"intro.html","id":"whats-regularization-and-whats-the-difference-between-l1-and-l2-regularization","chapter":"2 ML Models","heading":"2.7 What’s Regularization? and what’s the difference between L1 and L2 regularization?","text":"Regularization machine learning process regularizing parameters constrain, regularizes, shrinks coefficient estimates towards zero. words, technique discourages learning complex flexible model, avoiding risk Overfitting. Regularization basically adds penalty model complexity increases can help avoid overfitting.","code":""},{"path":"intro.html","id":"ridge-regression","chapter":"2 ML Models","heading":"2.7.1 Ridge Regression","text":"Ridge regression, also known L2 Regularization, regression technique introduces small amount bias reduce overfitting. minimizing sum squared residuals plus penalty, penalty equal lambda times slope squared. Lambda refers severity penalty.Without penalty, line best fit steeper slope, means sensitive small changes X. introducing penalty, line best fit becomes less sensitive small changes X. idea behind ridge regression.","code":""},{"path":"intro.html","id":"lasso-regression","chapter":"2 ML Models","heading":"2.7.2 Lasso Regression","text":"Lasso Regression, also known L1 Regularization, similar Ridge regression. difference penalty calculated absolute value slope instead.","code":""},{"path":"intro.html","id":"can-we-use-l1-regularization-for-feature-selection","chapter":"2 ML Models","heading":"2.8 Can we use L1 regularization for feature selection?","text":"Yes, nature L1 regularization lead sparse coefficients features. Feature selection can done keeping features non-zero coefficients.","code":""},{"path":"intro.html","id":"when-do-we-need-to-perform-feature-normalization-for-linear-models-when-its-okay-not-to-do-it","chapter":"2 ML Models","heading":"2.9 When do we need to perform feature normalization for linear models? When it’s okay not to do it?","text":"Feature normalization necessary L1 L2 regularizations. idea methods penalize features relatively equally. can’t done effectively every feature scaled differently.Linear regression without regularization techniques can used without feature normalization. Also, regularization can help make analytical solution stable, — adds regularization matrix feature matrix inverting .","code":""},{"path":"intro.html","id":"logistic-regression","chapter":"2 ML Models","heading":"2.10 Logistic Regression","text":"Logistic Regression classification technique also finds ‘line best fit’. However, unlike linear regression line best fit found using least squares, logistic regression finds line (logistic curve) best fit using maximum likelihood. done y value can one zero.Logistic regression similar linear regression used model probability discrete number outcomes, typically two. example, might want predict whether person alive dead given age.glance, logistic regression sounds much complicated linear regression, really one extra step.First, calculate score using equation similar equation line best fit linear regression.extra step feeding score previously calculated sigmoid function get probability return. probability can converted binary output, either 1 0.find weights initial equation calculate score, methods like gradient descent maximum likelihood used. Since ’s beyond scope article, won’t go much detail, now know works!","code":""},{"path":"intro.html","id":"what-is-logistic-regression-or-state-an-example-when-you-have-used-logistic-regression-recently.","chapter":"2 ML Models","heading":"2.11 What is logistic regression? Or State an example when you have used logistic regression recently.","text":"Logistic Regression often referred logit model technique predict binary outcome linear combination predictor variables. example, want predict whether particular political leader win election . case, outcome prediction binary .e. 0 1 (Win/Lose). predictor variables amount money spent election campaigning particular candidate, amount time spent campaigning, etc.","code":""},{"path":"metrics.html","id":"metrics","chapter":"3 Metrics","heading":"3 Metrics","text":"review existing methods.","code":""},{"path":"clustering.html","id":"clustering","chapter":"4 Clustering","heading":"4 Clustering","text":"describe methods chapter.Math can added body using usual syntax like ","code":""},{"path":"clustering.html","id":"math-example","chapter":"4 Clustering","heading":"4.1 math example","text":"\\(p\\) unknown expected around 1/3. Standard error approximated\\[\nSE = \\sqrt(\\frac{p(1-p)}{n}) \\approx \\sqrt{\\frac{1/3 (1 - 1/3)} {300}} = 0.027\n\\]can also use math footnotes like this1.approximate standard error 0.0272","code":""},{"path":"data-preprocessing.html","id":"data-preprocessing","chapter":"5 Data Preprocessing","heading":"5 Data Preprocessing","text":"significant applications demonstrated chapter.","code":""},{"path":"data-preprocessing.html","id":"example-one","chapter":"5 Data Preprocessing","heading":"5.1 Example one","text":"","code":""},{"path":"data-preprocessing.html","id":"example-two","chapter":"5 Data Preprocessing","heading":"5.2 Example two","text":"","code":""},{"path":"neural-networks.html","id":"neural-networks","chapter":"6 Neural Networks","heading":"6 Neural Networks","text":"finished nice book.","code":""},{"path":"sql.html","id":"sql","chapter":"7 SQL","heading":"7 SQL","text":"finished nice book.","code":""},{"path":"references.html","id":"references","chapter":"References","heading":"References","text":"","code":""}]
